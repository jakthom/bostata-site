"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[9450],{6029:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"staying-fresh-with-freshness-tables","metadata":{"permalink":"/staying-fresh-with-freshness-tables","source":"@site/blog/2021-12-19-staying-fresh-with-freshness-tables.md","title":"Staying Fresh with Freshness Tables","description":"Without freshness, why should anyone trust you or your systems? Why should they pay your company money or take your recommendations and \\"science\\" seriously? Why should they spend their time building on your systems or data warehouse?","date":"2021-12-19T00:00:00.000Z","formattedDate":"December 19, 2021","tags":[],"readingTime":8.11,"truncated":true,"authors":[],"frontMatter":{"slug":"staying-fresh-with-freshness-tables","title":"Staying Fresh with Freshness Tables","hide_table_of_contents":true},"nextItem":{"title":"268 Billion Events With Snowplow and Snowflake at Cargurus","permalink":"/268-billion-events-with-snowplow-snowflake-at-cargurus"}},"content":"Without freshness, why should anyone trust you or your systems? Why should they pay your company money or take your recommendations and \\"science\\" seriously? Why should they spend their time building on your systems or data warehouse?\\n\\n\x3c!--truncate--\x3e\\n\\n# But Freshness is Really Hard.\\n\\n\\nData warehouses store data... of different shapes and sizes.... sourced from different places... at different frequencies... and modeled in different ways. It\'s the data team\'s job to make that process transparent, reliable, and understandable.\\n\\nAs the use of data ramps in an organization (often rapidly), conversations tend to go from the initial \ud83e\udd2f\ud83d\ude4c\ud83d\ude2e to \\"how quickly is my data loaded?\\".\\n\\n### All-too-familiar questions look something like this:\\n\\n**How often do we load our Postgres instances?**\\n\\n\\"Every fifteen minutes.\\"\\n\\n**How about logs?**\\n\\n\\"Daily.\\"\\n\\n**We have an event stream right?**\\n\\n\\"Of course! Events land in Snowflake every three minutes.\\"\\n\\n**How about the Salesforce and Hubspot integrations?**\\n\\n\\"Hubspot is loaded hourly. We ran into API limits with Salesforce so that\'s loaded every few hours. Less often if we exceed quotas and the retries kick in.\\"\\n\\n**Did we ever start loading Greenhouse? I want to dig into hiring funnels.**\\n\\n\\"Sure did. Applications, candidates, stages, scorecards are all loaded twice per day. Rejection reasons are once per day.\\"\\n\\n**What about our client data feeds?**\\n\\n\\"We grab them as often as we can, but most land on Thursday or Friday. And they never come in on the weekend. Manually-curated ones always go dark in December.\\"\\n\\n**This is great. How about our DBT models?**\\n\\n\\"Table materializations are run twice per day. Incremental models run every hour. Our views pass-through to the source tables, so they are as fresh as the source tables.\\"\\n\\n\\"But we manually refresh the materialized views with Airflow every thirty minutes...\\"\\n\\n\\"And we are playing with Materialize on Kafka, so that\'s usually within a few hundred ms...\\"\\n\\n\\"And there are couple Snowflake tasks...\\"\\n\\n*....and the list goes on.*\\n\\nThis conversation quickly grows as teams, data sources, technologies, data models, and stakeholder value grows. Engineers who were once thrilled to build new systems quickly become burdened by all the freshness. Managers who were once able to recruit and onboard team members or advance initiatives become the context bottleneck. \\"Ms. Manager, is this table fresh? How about this one? How often can I expect data to be here?\\"\\n\\n# Measuring Freshness with Record-Level Metadata is OK.\\n\\nOnce upon a time database people kept track of record-level metadata with createdAt, updatedAt, and deletedAt columns. And fun stuff like database triggers, functions, audit tables, chewing on the WAL, whatever.\\n\\nThis is all dandy! But only for the source database.\\n\\nSo data warehouse people added \\"warehouse\\" or \\"system\\" record-level metadata columns: \\"_updated_at\\", \\"dw_updated_at\\", \\"_loaded_at\\", \\"fivetran_synced\\" and the like. Sprinkle in some column defaults and you\'re well on your way.\\n\\n[Stitch](https://www.stitchdata.com/docs/replication/loading/system-tables-columns) and [Fivetran](https://fivetran.com/docs/getting-started/system-columns-and-tables) do this. It works! Querying \\"select max(fivetran_synced)\\" for all tables lets you quickly see the last time they were loaded. **Fresh.**\\n\\nDBT uses [Freshness Checks](https://docs.getdbt.com/reference/resource-properties/freshness) to \\"select max(loaded_at)\\" sources. And provides a handy Source Freshness UI to give a visual representation of source status. **Definitely fresh.**\\n\\n***But there are some downsides to all this freshness. Especially in columnar databases.***\\n\\nAs tables grow in size a \\"select max(loaded_at)\\" is super wasteful. These tables are rarely partitioned or clustered on load-metadata columns. And if tables are initially clustered by \\"loaded at\\", they probably won\'t be for very long. Freshness checks will therefore be less efficient (and slower) as the table gets larger, and you will spend more and more money trying to figure out how fresh things are. With limited success. **Not fresh.**\\n\\nIf you\'re using BigQuery, each freshness check will use a job slot. Which is ok, until you scale. Or have more people checking dashboards. Or run things more frequently. A freshness check that accelerates your systems bouncing off BigQuery concurrency limits is not fresh. Batch Priority freshness checks? **Not fresh!**\\n\\nIf you\'re using Redshift, each freshness check will steal resources that could and should be used to load, model, and serve data. **A freshness check that requires WLM tuning is definitely not fresh.**\\n\\nIf freshness checks are being executed with Airflow sensors, I\'m going to conveniently avoid the subject. **It\'s not fresh.**\\n\\nAnd last but certainly not least: just because a table was loaded at T1 doesn\'t mean it was entirely up to date at that time. **Fresh? Maybe. But also maybe not.**\\n\\n# Measuring Freshness with Freshness Tables is Fresh.\\n\\nWhile certainly important and useful, record-level metadata columns are not always awesome. They certainly serve a purpose but freshness tables are better.\\n\\nFreshness tables are not complicated and could look as simple as this:\\n\\n```\\nmetadata=# \\\\d freshness.tbl_freshness\\n                                               Table \\"freshness.tbl_freshness\\"\\n        Column        |           Type           | Collation | Nullable |                       Default\\n----------------------+--------------------------+-----------+----------+-----------------------------------------------------\\n id                   | integer                  |           | not null | nextval(\'freshness.tbl_freshness_id_seq\'::regclass)\\n fresh_at             | timestamp with time zone |           | not null |\\n fqn                  | text                     |           | not null |\\n table_catalog        | text                     |           | not null |\\n table_schema         | text                     |           | not null |\\n table_name           | text                     |           | not null |\\n system               | text                     |           | not null |\\n job_started_at       | timestamp with time zone |           | not null |\\n job_ended_at         | timestamp with time zone |           | not null |\\n job_duration_seconds | double precision         |           | not null |\\n highwater_start      | timestamp with time zone |           | not null |\\n highwater_end        | timestamp with time zone |           | not null |\\n\\n\\nIndexes:\\n    \\"tbl_freshness_catalog_idx\\" btree (table_catalog)\\n    \\"tbl_freshness_fresh_at_idx\\" btree (fresh_at)\\n    \\"tbl_freshness_schema_idx\\" btree (table_schema)\\n    \\"tbl_freshness_table_idx\\" btree (table_name)\\n```\\n\\nSource freshness is definitely cool. But what about freshness at all other stages of the data dependency graph? What about staging, base/intermediary/backroom model, fact, dim/scd, and presentation-layer freshness? What about data application freshness, or the master data tier? What about scheduled query freshness?\\n\\nWhat about your metrics tier?\\n\\nMeasuring freshness across the entire graph is super important. Freshness tables let you do exactly that.\\n\\n\\n# How To Be Fresh\\n\\n### Track It.\\n\\nFreshness is hard but tracking freshness doesn\'t have to be. And you\'ll have to start somewhere.\\n\\nIt is not a great feat of engineering to collect metadata within ingestion systems and persist that meta after each load iteration. Load instrumentation is fresh (and you should probably be doing it anyways).\\n\\nIt\'s pretty easy to add [post-hooks](https://docs.getdbt.com/reference/resource-configs/pre-hook-post-hook) to DBT models. Calculating and persisting data model freshness is fresh.\\n\\nRefreshing a materialized view? Add a \\"INSERT INTO metadata.tbl_freshness values (select something interesting)\\" line to the end of the statement, wrap it in a transaction, and voila. Instafresh. (And you might have [this already](https://docs.snowflake.com/en/sql-reference/functions/materialized_view_refresh_history.html).)\\n\\n\\nUse Airflow to manage data ingestion and transformation? Great! The power of [on_success_callback](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/index.html#airflow.models.Base) and [context](https://composed.blog/airflow/execute-context) is already yours. Fresh.\\n\\nAt a loss for how to track freshness? Even a webhook-to-freshness-table can be fresh!\\n\\n### Rock It.\\n\\nBelieve it or not, many analysts and downstream engineers will self-service if they have low-friction ways for doing so.\\n\\nUnfortunately they usually have to go to N different places, where N is the number of sources or systems in operation. Requiring stakeholders to know data systems inside-and-out, and requiring them to waste their time tracking down architectural decisions, is not fresh.\\n\\nFreshness tables give people who are good at analytics the capability to do analytics on their insights. Meta? Yup. Powerful? Also yup. Efficient? Definitely.\\n\\nMy current employer has a single table of 31,630,551 freshness records, across thousands of distinct table FQN\'s and hundreds of sources. We know exactly when our data assets were historically fresh. We also know various platform upgrades successfully decreased time to insight, and which sources or data transformations were most impacted by those upgrades.\\n\\nIngestion and modeling systems currently persist freshness at a rate of ~10,000 records every hour, and we have several Tableau dashboards for tracking freshness deltas across categories.\\n\\nLoad and modeling optimizations are quickly surfaced, and the freshness feedback loop is on autopilot. It\'s fresh. And we rock it.\\n\\n# Why Be Fresh?\\n\\n### Freshness is the goal.\\n\\nHistorical data is good, but fresh data is better. Especially when it is being used to make relevant, time-sensitive decisions. Known-and-communicated-fresh data? The best!\\n\\n### Freshness is trust.\\n\\nWhen you\'re fresh and you can prove it, you retain significant trust. When you\'re fresh and others can prove it for you, it\'s even better. Freshness tables exposed to stakeholders allows them to tell you you\'re fresh (or otherwise).\\n\\n### Freshness is seeing systems get better or worse. And learning fast.\\n\\nData systems are complex, and the most important thing you can do when building complex systems is provide visibility into them. When anyone can debug history at any point in time, it\'s fresh. When you can prove, using historical freshness data, that engineering work is decreasing time to insight, it\'s fresh.\\n\\nWhen you can see freshness getting worse as volume increases, new systems come online, or new platform requirements get added, it\'s fresh. When you can alert on freshness thresholds...... you get the idea.\\n\\n### Freshness is empowering.\\n\\nIf it\'s not clear by now, knowing what systems are doing and what tables are fresh is incredibly empowering. It enables work prioritization, continuous system insights, proactive solutions, and innovation. Freshness facilitates data SLO\'s and SLA\'s, helps frame conversations around a common point of reference, and a whole lot more.\\n\\n# Use Freshness To Keep Getting Fresh[r].\\n\\nAfter seeing the past 6-7 months of freshness data I cannot stop thinking about the implications of it.\\n\\nCommunicating context in dashboards is fresh.\\n\\nGetting away from time dependencies and proactively triggering model sub-graphs based on the knowledge that upstream dependencies are known-fresh is... fresh.\\n\\nTracking and reporting on time to insight is fresh.\\n\\nBaselining various cost optimizations (Snowflake warehouse sizing?) is fresh.\\n\\nAnd there are many more thoughts to come.\\n\\n# So stay fresh out there."},{"id":"268-billion-events-with-snowplow-snowflake-at-cargurus","metadata":{"permalink":"/268-billion-events-with-snowplow-snowflake-at-cargurus","source":"@site/blog/2020-06-15-snowplow-snowflake-at-cargurus.md","title":"268 Billion Events With Snowplow and Snowflake at Cargurus","description":"Two years ago we set up Open-Source Snowplow at CarGurus to fulfill a need for self-managed client-side instrumentation. Since that time it has become incredibly impactful for the entire company and has scaled significantly beyond what was originally envisioned. The following is an overview of why we set up the system, our experience with it, what we have learned, and where we see it continuing to go.","date":"2020-06-15T00:00:00.000Z","formattedDate":"June 15, 2020","tags":[{"label":"snowplow","permalink":"/tags/snowplow"},{"label":"snowflake","permalink":"/tags/snowflake"},{"label":"cargurus","permalink":"/tags/cargurus"}],"readingTime":16.59,"truncated":true,"authors":[{"name":"Jake"}],"frontMatter":{"slug":"268-billion-events-with-snowplow-snowflake-at-cargurus","title":"268 Billion Events With Snowplow and Snowflake at Cargurus","authors":{"name":"Jake"},"tags":["snowplow","snowflake","cargurus"],"hide_table_of_contents":true},"prevItem":{"title":"Staying Fresh with Freshness Tables","permalink":"/staying-fresh-with-freshness-tables"},"nextItem":{"title":"How To Install and Configure SnowSQL","permalink":"/how-to-install-and-configure-snowsql"}},"content":"Two years ago we set up Open-Source Snowplow at CarGurus to fulfill a need for self-managed client-side instrumentation. Since that time it has become incredibly impactful for the entire company and has scaled significantly beyond what was originally envisioned. The following is an overview of why we set up the system, our experience with it, what we have learned, and where we see it continuing to go.\\n\\n\\nThis post is quite long, so before getting too far into the details....\\n\\n\x3c!--truncate--\x3e\\n\\n# The Stats At Time of Writing\\n\\nTotal events collected: **>268 billion**\\n\\nData collected: > **1.5pb (uncompressed)**\\n\\nEvent volume: **~ 1 billion events/day**\\n\\nMax Daily Throughput: **~15k events/second (sustained during peak hours)**\\n\\nDistinct events: **hundreds and hundreds**\\n\\nInfrastructure upgrade duration: **less than two minutes with zero downtime**\\n\\nDistinct sites with Snowplow tracking: **>180**\\n\\nSo let\'s dive in.\\n\\n# Why we Snowplow\\n\\nWe chose Snowplow instead of building our own event tracking system, and plan to stick with it for the foreseeable future. Why?\\n\\n**We wanted to independently manage and scale data collection systems.**\\n\\nA conversation early in my tenure at CarGurus made a topic abundantly clear: many teams had a strong desire to separate concerns. At that time the company was growing quickly and demands were rapidly changing. Analysts and engineers were joining, onboarding fast, and asking completely new questions of our data. It was all-too-common to tell them their questions simply could not be answered due to the additional load tracking would place on production systems.\\nAfter being personally involved with a couple event-volume-related site outages, it was evident we needed to rethink data collection systems. We needed to be able to scale data collection infrastructure completely separately from application infrastructure, and we needed to significantly isolate the blast radius if something went wrong.\\n\\nSnowplow was a strong \u2705.\\n\\n**We wanted to modernize event collection systems.**\\n\\nAnother requirement of choosing Snowplow was its ability to quickly and effectively introduce a lambda architecture into our analytics stack. As customer needs and demands evolved, we found that having both a real-time, low-latency (on the order of hundreds of milliseconds) component of the event pipeline as well as a batch-based, higher-latency (on the order of minutes) component gave us significant flexibility to proactively fulfill stakeholder asks before they surfaced.\\n\\nOur pipelines are set up in AWS with Kinesis as transport and Snowflake as long-term data storage. As stakeholders need real-time access to enriched data, Kinesis is the go-to location. If stakeholders need access to historical event data or want to augment it with other sources in the data warehouse, Snowflake is the place to go.\\n\\nSnowplow allowed us to introduce new ideas and methodologies and fulfill both point-in-time and future stakeholder needs. \u2705\\n\\n### The world of third-party tracking is rapidly changing and will continue to.\\n\\nAs anyone involved in the world of client-side analytics is probably well-aware of, third-party tracking is increasingly going away. Initiatives like this and this and this are fantastic for online privacy (and commendable), but have major implications for tracking and analytics. With an eye towards the future we knew we needed to decrease reliance on Google Analytics, Adobe, Heap, and all other similar third-party tracking systems if we wanted full insight into web traffic. We wanted 100% of site behavior instead of sampled GA or ad-blocked Adobe. And we wanted to own the resulting data.\\n\\nSnowplow enabled us to track first-party web activity and regain full visibility into usage behavior on our site. \u2705\\n\\n### Snowplow sets a high bar for fault-tolerance, redundancy, and durability.\\n\\nWhen set up properly, the Snowplow infrastructure is extremely fault-tolerant. We set up ours in AWS and have not regretted it whatsoever.\\nJavascript tracking code buffers unsent events locally in the case of collection infrastructure being down. AWS application load balancers are multi-AZ and AWS maintains a good SLA for them. All access logs are retained for a period of time in case the load balancer is up but the rest of the system is down. Kinesis replicates data across three AZ\'s by default, and retention can be configured up to 168 hours in the case of mid-pipeline enrichment outages. S3 is highly durable and highly available by default. And the list goes on....\\n\\n### We wanted to own all data that was generated.\\n\\nAs mentioned above, if we were going to invest time and money building scalable event-collection infrastructure, we simply had to own the outcome. It is a significant investment to get large-scale infrastructure and instrumentation off the ground, and it\'s hard to do tracking in a way that doesn\'t impose a burden on analysts down the road (think Google Analytics events).\\nWe didn\'t want to deal with API rate limiting when attempting to recover our data. We didn\'t want to be tied to BigQuery, and we didn\'t want our customers\' data flowing through other companies\' systems.\\n\\n\\n### We needed to keep data quality high.\\n\\nWhen building out data collection and processing systems, it\'s one thing to set up infrastructure. It\'s a completely different story to keep quality high as priorities, teams, and customer demands shift. It\'s even harder to impose more work in an attempt to keep data \\"clean\\".\\n\\nThe way snowplow handles event validation and stream-redirection was a perfect fit in our case. After setting the system up we quickly found that leveraging Self-Describing Events and the associated jsonschemas empowered us to enforce data quality in a very low-friction way. Whenever events start flowing (with a bad push, etc) that don\'t adhere to the implementing party\'s jsonschema, they are redirected out of the \\"good\\" path for immediate review and recovery. Self-describing events (and jsonschemas) also gave us the ability to do some pretty neat automation, which will be covered later.\\n\\n### The world of compliance is changing rapidly and we needed the ability to audit every piece of information.\\n\\nSince we first set up Snowplow the world has been forced to navigate two big data-privacy hurdles: GDPR and CCPA. Since we own and operate all collection infrastructure we did not need to get a third-party vendor DPA. Snowplow gives a significant amount of flexibility for enriching and redirecting PII, as well as configurable pseudonymization. It\'s good, it \\"just works\\", and it has saved us a considerable amount of time.\\n\\nWe\'ve also spent a significant amount of time persisting full system lineage when splitting events into Snowflake which makes it easy to fully adhere to privacy legislation and associated requirements.\\n\\n### We wanted to build, guide, and get out of the way.\\n\\nMy favorite part about programming and tech is building (machine) processes, cultivating good (human) habits, and getting out of the way. An early challenge was maintaining clear, consistent definitions of data between frontend engineers (the implementer of tracking) and analysts (the stakeholders of said tracking). Event data was previously tracked, flowed through a number of different systems, and was often renamed (sometimes several times) when it finally landed in Snowflake.. Snowplow\'s self-describing events have allowed us to shift many conversations away from data pipeline engineers and back to where they should be taking place. I was able to back away since stakeholders and implementers were able to approach important conversations from a point of shared, consistent understanding.\\n\\n### Last but not least, we needed to hit the ground running.\\n\\nWhile we may have had time to build out our own system from scratch, why build when you can lean on the backs of giants? The community of people contributing to Snowplow has grown significantly since the first time I implemented it, and people all over the world have run into almost every challenge imaginable. As fun as it would have been to deconstruct the system and build and implement our own, it simply wasn\'t worth it. Our stakeholders had questions that they wanted answered \\"yesterday\\" and their needs were our priority #1.\\n\\nSnowplow enabled us to hit the ground running and answer real business questions \\"now\\". \u2705\\n\\n\\n# Our Journey\\n\\n## Phase 1: Keep It Simple. Get It Running. Prove the System\'s Worth.\\n\\nWhen Snowplow was first rolled out there was a single mission in mind: make sure it was a good organizational fit. We had systems in place with some functional overlap already, so we had to make sure we weren\'t introducing a new system simply........ for the sake of introducing a new system. Or staking claim in others\' territory. The system had to work. It had to answer questions that had been previously-unanswerable. It had to be robust. And it had to solve real-life challenges.\\n\\n### We very consciously chose to:\\n\\n**Automate all infrastructure.** We wanted to scale, upgrade, and roll out entirely new sets of infrastructure fast. Everything was Terraformed and infrastructure was 100% immutable.\\n\\n**Keep it simple.** We could have gone straight to k8s or autoscaling clusters or ecs or eks or any other new (admittedly attractive) thing. But we didn\'t. We wanted to see how the system itself would perform and scale.\\n\\n**Focus on the stakeholder.** The only way this initiative was going to work was if others felt ownership of it, it decreased their pain, it empowered them to do their jobs better, or a combination of all. Instead of focusing on a hip/cool/hot/new system we focused on answering the questions at hand.\\n\\n## Phase 2: Ramp Adoption. Answer More Questions.\\n\\nOnce the system was set up and a minimal set of events were implemented across our site, we started really putting the system to work. We went around the company figuring out what questions people had been historically struggling to answer, and then helped implement the necessary tracking to answer these questions. We also pushed the system in other ways, such as seeing how effective it was for third-party tracking on other websites, redirect tracking, no-js tracking, and more.\\n\\nWe originally rolled out most of the tracking via structured events. While great for adoption, it was evident things would quickly get out of hand before realizing the full value of the system if we continued down this path. Json-encoded strings with arbitrary values were being passed as se_property. Event definitions were unclear and quickly inconsistent. Snowflake query duration (and therefore cost) was going through the roof as people tried to pull data out of a single poorly-clustered \\"struct_events\\" table. And more.\\n\\nWhile we were able to answer questions, it was clear this methodology of doing so would be unsustainable and/or expensive long term.\\n\\n## Phase 3: Go All-In on Mandating Data Quality.\\n\\nAfter quickly learning the pain points associated with struct-event tracking in a company our size we had a decision to make: **go all-in on self-describing events or bust.**\\n\\nAdmittedly, mandating that events only get successfully delivered when accompanied by a jsonschema is a bit of overhead that is passed to the implementer. In this case, quality comes at a small time cost. But said cost is more than worth it.\\n\\n**Why it\'s worth using schemas:**\\n\\n- Data can be explicitly defined and versioned. Schema evolution is possible without blowing up the world of analytics.\\n- Self-describing events can be independently monitored and alerted on.\\n- Incoming events are validated and redirected in-transit.\\n- Jsonschemas establish consistency and empower 1:1 communication.\\n- Significant downstream automation can be built on top of schemas. Auto-migrating tables, splitting/deduplicating events, and intelligently clustering tables in snowflake are only possible with self-describing events.\\n- \\"Bad\\" event volume can be alerted on and directed to the respective team.\\n\\n## Phase 4: Solidify Infrastructure. Add Necessary Complexity. Automate Data Engineering Toil Away.\\n\\nOnce we knew the system would work and that it was a good organizational fit, the next step was to make the supporting infrastructure rock-solid.\\n\\nOriginally setting up the system in the simplest manner possible meant that we willingly accepted tradeoffs and some risk. We had to make sure we had enough machines behind the load balancer to service traffic at any point in time. We had to closely (and sometimes manually!) monitor the enrichment and s3 loader machines. And we had to manually scale supporting infrastructure when necessary. From a cost and utilization perspective, we ran the system slightly over-provisioned during peak volume and significantly over-provisioned during the slowest points of the day. The simplicity came at a cost tradeoff, but again - there\'s very little point in optimizing a system that you\'re not going to continue using.\\n\\nAs we became more confident in the long-term direction of this project it was time to buckle down and set up all infrastructure in a way that we could step away from. This ultimately meant containerizing all systems, introducing auto-scaling groups with reasonable scaling policies, auto-scaling dynamodb, figuring out how to reasonably auto-scale kinesis, and a number of other not-too-complicated-but-incredibly-important devops things. We decided to run EC2 ASG\'s of flatcar container linux vs running the containers on ECS, EKS, or a k8s cluster, though one of those will probably be in our future.\\n\\nAnother critical aspect of this automation was the implementation of a load/deduplicate/split/auto-cluster system. We have pipelines geolocated all over the world and load all data into Snowflake. By leveraging self-describing event json-schemas mandated in Phase 2, we were able to auto-migrate Snowflake tables, auto-split events into said tables, efficiently deduplicate events, strategically cluster, and much much more.\\n\\nThe automation here has saved us considerable amounts of time and money, and has allowed us to go incredibly far by automating ourselves away.\\n\\n## Phase 5: Decrease Implementation Overhead. Don\'t Sacrifice Data Quality.\\n\\nThis is the phase we are in currently and it\'s a very important one. As a data engineer, if you get data quality wrong (even unintentionally!), it can easily lead a company in a bad direction and will create massive problems that take years to resolve. We did not want to make these mistakes, so we quickly pivoted towards strictly prioritizing data quality. But we believe that\'s not the end of the story. If we can mandate data quality while making schemas as easy to implement, deploy, evolve, and monitor as not having them at all, we\'ll consider it a massive success. We also strive towards establishing clear, consistent, sharable definitions for all and make all definitions easily searchable.\\n\\nAt this point we are well on our way to achieving these goals. But we aren\'t quite there yet.\\n\\n# What we have done a bit differently\\n\\nThe snowplow stream collector, the stream enricher, and the s3 loader are basically out-of-the-box and have been configured to suit our needs and volume. But we\'ve gone a slightly differently direction with other core aspects of the system.\\n\\nWe decided not to use the snowplow snowflake loader largely due to avoiding additional dependencies, needing more flexibility, and desiring to process and store data consistently to other internal systems. We have a significant amount of professional experience with Snowflake administration/automation, and saw opportunity to make our lives a bit easier long term if we rolled this portion of the pipeline ourselves.\\n\\nWe decided not to use the iglu schema repository (but do use jsonschemas for self-described events!) due to identifying numerous opportunities at this level. This functionality may be covered in another future blog post.\\n\\nWe love Graylog, fluent-bit, and a handful of other tools. So we\'ve incorporated them in useful ways.\\n\\n# What we have learned along the way\\n\\n### Stakeholder empowerment is #1.\\n\\nI cannot stress this enough: the only way to make initiatives like this work is by taking a customer-focused, stakeholder-oriented approach. As a data engineer my customers are typically analysts or various business intelligence initiatives. But as data maturity grows in the organization the word \\"stakeholder\\" evolves to also include other internal systems, and then SEO optimization, potentially strategic partnerships with other orgs, external customers, and much more.\\n\\nBy running Snowplow and leaning into significant Snowflake automation my role has become less gatekeeper and more facilitator, working alongside stakeholders to achieve the respective end goal.\\n\\n### Infrastructure and system automation is #2.\\n\\nSetting up and thoroughly understanding the Snowplow stack on AWS is non-trivial. There\'s a significant number of moving pieces and a deep understanding of Kinesis internals is required to manage it well. We automated everything from the very start so spinning up and properly configuring 70+ AWS resources per pipeline becomes (almost) trivial.\\n\\nAnother benefit of full infrastructure automation is the ability to shut systems down cleanly when we don\'t need them anymore, and make code central to infrastructure rollouts.\\n\\n### Explicitly mandating data quality has numerous benefits. It may come at a cost, but it doesn\'t have to.\\n\\nLeaning into 100% self-describing Snowplow events admittedly comes with operational overhead. \\"Bad\\" must scale as cleanly as \\"good\\" in the case that a high number of events are redirected. Otherwise, the \\"pipes\\" will become constricted and the entire system will back up. Event schemas must be properly-validated json before being deployed into production pipelines. Front-end engineers must be personally invested in helping to mandate data quality, and using jsonschemas to validate each piece of instrumentation is new cognitive load.\\n\\nWe\'ve leaned into automation and/or internal tooling here as well to help reduce this overhead, but there\'s much further to go.\\n\\n### The Snowplow+Snowflake combination scales extremely well. It has empowered us to answer many previously-unanswerable questions and has opened up numerous doors of opportunity.\\n\\nI have personally run open-source Snowplow analytics a large number of times with AWS Redshift as a data warehouse. While functional, it quickly becomes a burden to maintain and scale. The Snowplow+Snowflake combination is extremely effectively and extremely powerful. And scales well, even at CarGurus\' volume.\\n\\n# Where we see it continuing to go in the future\\n\\n### Leverage the stream, Luke\\n\\nThere\'s so much more opportunity to leverage data in-transit that we\'ve barely cracked the surface of. Streaming databases such as Materialize and KSQL show massive promise for continuing to decrease time-to-insight and are some of the most exciting data projects since PipelineDB. I\'m thrilled at the ability to bolt functionality onto an existing system (vs. re-architecting it from scratch as demands change) and am very interested to see what the low-latency future holds.\\n\\n### Continued focus on auditability, provenance, and data governance.\\n\\nHave I mentioned self-describing events with jsonschemas are amazing? Another significant benefit of them is the ability to cleanly, quickly, and effectively track provenance from upstream systems, all the way through the pipeline, to a data warehouse. Provenance is quite easy.\\n\\nWhen it comes to data governance and auditing, we took numerous intentional steps in our Snowflake loader to make finding and purging data upstream doable. Snowflake\'s  METADATA$FILENAME  and  METADATA$FILE_ROW_NUMBER  functionality means that if anyone requests removal of data (per GDPR) or requests data not be sold (per CCPA), it can be purged or excluded from both Snowflake as well as upstream flat files sitting in S3/Glacier/etc.\\n\\nContinue to empower stakeholders, developers, analysts\\n\\nLast but certainly not least, identifying and eliminating points of misunderstanding, cognitive load, or implementation/development challenges remains top priority. \\"Stakeholders\\" may be fellow engineers, they may be analysts whose job is to use the data we\'re collecting and warehousing, or they may be product owners whose job and decision-making process relies on our data.\\n\\nWe\'ll continue to push forward to make schema creation, discovery, evolution, and management more intuitive and fun.\\n\\nWe\'ll continue to make real-time analytics a reality.\\n\\nWe\'ll continue to make event-level observability and anomaly detection a critical part of instrumentation.\\n\\nWe\'ll continue to push forward with automating ourselves out of the way.\\n\\nAnd we\'ll keep trying our best to make others\' lives easier.\\n\\n# In Conclusion\\n\\nIf there\'s any confusion by this point, the Snowplow and Snowflake combination has worked incredibly well for CarGurus. The company has leveraged these systems to blow open doors of opportunity and the system has proven itself time and time again over the past couple years.\\n\\nLooking backwards, I\'m quite happy with the rollout and have certainly learned a lot.\\n\\nLooking forward I see only pure potential."},{"id":"how-to-install-and-configure-snowsql","metadata":{"permalink":"/how-to-install-and-configure-snowsql","source":"@site/blog/2019-12-06-how-to-install-and-configure-snowsql/index.md","title":"How To Install and Configure SnowSQL","description":"SnowSQL is the command-line interface for accessing your Snowflake instance.","date":"2019-12-06T00:00:00.000Z","formattedDate":"December 6, 2019","tags":[],"readingTime":4.38,"truncated":true,"authors":[],"frontMatter":{"slug":"how-to-install-and-configure-snowsql","title":"How To Install and Configure SnowSQL","hide_table_of_contents":true},"prevItem":{"title":"268 Billion Events With Snowplow and Snowflake at Cargurus","permalink":"/268-billion-events-with-snowplow-snowflake-at-cargurus"},"nextItem":{"title":"GDPR for Engineers - What You Need to Know","permalink":"/gdpr-for-engineers-what-you-need-to-know"}},"content":"[SnowSQL](https://docs.snowflake.com/en/user-guide/snowsql.html) is the command-line interface for accessing your Snowflake instance.\\n\\nThe following is a quick \\"how to\\" guide for setting it up.\\n\\n\x3c!--truncate--\x3e\\n\\n### Installation\\n\\nAfter logging into your Snowflake web interface, the SnowSQL installer is available via `Help` -> `Download`:\\n\\n![download](01_help_download.png)\\n\\nYou\'ll need to select the appropriate version for your machine:\\n\\n![cli](02_snowcli.png)\\n\\n..and install it:\\n\\n![install](03a_install.png)\\n\\n![install-success](03b_install.png)\\n\\nTo verify installation, simply open a terminal window and run snowsql. If installed properly, you will receive a list of connection and option flags:\\n\\n```\\n$ snowsql\\n    Usage: snowsql [OPTIONS]\\n\\n    Options:\\n      -a, --accountname TEXT          Name assigned to your Snowflake account. If\\n                                      you are not on us-west-2 or AWS deployement,\\n                                      append the region and platform to the end,\\n                                      e.g., <account>.<region> or\\n                                      <account>.<region>.<platform>Honors\\n                                      $SNOWSQL_ACCOUNT.\\n      -u, --username TEXT             Username to connect to Snowflake. Honors\\n                                      $SNOWSQL_USER.\\n      -d, --dbname TEXT               Database to use. Honors $SNOWSQL_DATABASE.\\n      -s, --schemaname TEXT           Schema in the database to use. Honors\\n                                      $SNOWSQL_SCHEMA.\\n      -r, --rolename TEXT             Role name to use. Honors $SNOWSQL_ROLE.\\n      -w, --warehouse TEXT            Warehouse to use. Honors $SNOWSQL_WAREHOUSE.\\n      -h, --host TEXT                 Host address for the connection. Honors\\n                                      $SNOWSQL_HOST.\\n      -p, --port INTEGER              Port number for the connection. Honors\\n                                      $SNOWSQL_PORT.\\n      --region TEXT                   (DEPRECATED) Append the region or any sub\\n                                      domains before snowflakecomputing.com to the\\n                                      end of accountname parameter after a dot.\\n                                      e.g., accountname=<account>.<region>\\n      -m, --mfa-passcode TEXT         Token to use for multi-factor authentication\\n                                      (MFA)\\n      --mfa-passcode-in-password      Appends the MFA passcode to the end of the\\n                                      password.\\n      --abort-detached-query          Aborts a query if the connection between the\\n                                      client and server is lost. By default, it\\n                                      won\'t abort even if the connection is lost.\\n      --probe-connection              Test connectivity to Snowflake. This option\\n                                      is mainly used to print out the TLS/SSL\\n                                      certificate chain.\\n      --proxy-host TEXT               (DEPRECATED. Use HTTPS_PROXY and HTTP_PROXY\\n                                      environment variables.) Proxy server\\n                                      hostname. Honors $SNOWSQL_PROXY_HOST.\\n      --proxy-port INTEGER            (DEPRECATED. Use HTTPS_PROXY and HTTP_PROXY\\n                                      environment variables.) Proxy server port\\n                                      number. Honors $SNOWSQL_PROXY_PORT.\\n      --proxy-user TEXT               (DEPRECATED. Use HTTPS_PROXY and HTTP_PROXY\\n                                      environment variables.) Proxy server\\n                                      username. Honors $SNOWSQL_PROXY_USER. Set\\n                                      $SNOWSQL_PROXY_PWD for the proxy server\\n                                      password.\\n      --authenticator TEXT            Authenticator: \'snowflake\',\\n                                      \'externalbrowser\' (to use any IdP and a web\\n                                      browser), or\\n                                      https://<your_okta_account_name>.okta.com\\n                                      (to use Okta natively).\\n      -v, --version                   Shows the current SnowSQL version, or uses a\\n                                      specific version if provided as a value.\\n      --noup                          Disables auto-upgrade for this run. If no\\n                                      version is specified for -v, the latest\\n                                      version in ~/.snowsql/ is used.\\n      -D, --variable TEXT             Sets a variable to be referred by &<var>. -D\\n                                      tablename=CENUSTRACKONE or --variable\\n                                      db_key=$DB_KEY\\n      -o, --option TEXT               Set SnowSQL options. See the options\\n                                      reference in the Snowflake documentation.\\n      -f, --filename PATH             File to execute.\\n      -q, --query TEXT                Query to execute.\\n      --config PATH                   Path and name of the SnowSQL configuration\\n                                      file. By default, ~/.snowsql/config.\\n      -P, --prompt                    Forces a password prompt. By default,\\n                                      $SNOWSQL_PWD is used to set the password.\\n      -M, --mfa-prompt                Forces a prompt for the second token for\\n                                      MFA.\\n      -c, --connection TEXT           Named set of connection parameters to use.\\n      --single-transaction            Connects with autocommit disabled. Wraps\\n                                      BEGIN/COMMIT around statements to execute\\n                                      them as a single transaction, ensuring all\\n                                      commands complete successfully or no change\\n                                      is applied.\\n      --private-key-path PATH         Path to private key file in PEM format used\\n                                      for key pair authentication. Private key\\n                                      file is required to be encrypted and\\n                                      passphrase is required to be specified in\\n                                      environment variable\\n                                      $SNOWSQL_PRIVATE_KEY_PASSPHRASE\\n      -U, --upgrade                   Force upgrade of SnowSQL to the latest\\n                                      version.\\n      -K, --client-session-keep-alive\\n                                      Keep the session active indefinitely, even\\n                                      if there is no activity from the user..\\n      --disable-request-pooling       Disable request pooling. This can help speed\\n                                      up connection failover\\n      -?, --help                      Show this message and exit.\\n```\\n\\n# Configuration\\n\\nAs indicated above, SnowSQL has a host of connection params and settings, and allows variable declaration and substitution. You won\'t need to be familiar with all the options to hit the ground running, but I definitely recommend leveraging ~/.snowsql/config to persist your connection details and personal preferences.\\n\\n### Add your connection details to the `connections` section of `~/.snowsql/config`.\\n\\nThe first few lines of your ~/.snowsql/config file should look like the following:\\n\\n```\\n[connections]\\naccountname = YOUR_ACCOUNT_NAME\\nusername = YOUR_USERNAME\\npassword = YOUR_PASSWORD\\n```\\n\\nPlease note! There are some caveats regarding quote-wrapping special characters and escaping quotes within passwords. For more information, please consult the [docs located here](https://docs.snowflake.com/en/user-guide/snowsql-config.html).\\n\\n### Add your personal preferences to the [options] section.\\n\\nI\'m OK with most of the configuration defaults, so the only option I typically modify is sfqid. It enables output of snowflake query id\'s in the summary, which can be quite helpful:\\n\\n```\\n    [options]\\n    sfqid = True\\n```\\n\\n**Want to be unfriendly? Add:**\\n\\n```\\nfriendly = False\\n```\\n\\n**Want to write to a specific log location? Add:**\\n\\n```\\nlog_file = your/path/to/log\\n```\\n\\n**Want to be difficult? Add:**\\n\\n```\\neditor = emacs\\n```\\n\\n(just kidding)\\n\\nYou get the idea. Your mileage may vary.\\n\\n\\n### Lock the file down to you and only you.\\n\\nIf you\'ve been paying attention, you have already realized sensitive credentials are stored in plaintext on your machine. This is not a reason to ?, and is similar to the postgres pgpass file. You\'ll want to lock it down.\\n\\n```chmod 400 ~/.snowsql/config```\\n\\n\\n# Summary\\n\\nSnowSQL is pretty easy to set up and start using, and you\'ll probably find it quickly becomes critical for development and database administration workflows. After installing the tool and becoming familiar with how it is configured and utilized, you\'ll be well on your way to a pleasant experience."},{"id":"gdpr-for-engineers-what-you-need-to-know","metadata":{"permalink":"/gdpr-for-engineers-what-you-need-to-know","source":"@site/blog/2019-05-13-gdpr-what-you-need-to-know.md","title":"GDPR for Engineers - What You Need to Know","description":"GDPR was approved by EU parliament on April 14, 2016, went into effect May 25, 2018, and impacts any business handling any personal data of any EU resident.","date":"2019-05-13T00:00:00.000Z","formattedDate":"May 13, 2019","tags":[{"label":"gdpr","permalink":"/tags/gdpr"},{"label":"engineers","permalink":"/tags/engineers"},{"label":"data privacy","permalink":"/tags/data-privacy"}],"readingTime":5.86,"truncated":true,"authors":[{"name":"Jake"}],"frontMatter":{"slug":"gdpr-for-engineers-what-you-need-to-know","title":"GDPR for Engineers - What You Need to Know","authors":{"name":"Jake"},"tags":["gdpr","engineers","data privacy"],"hide_table_of_contents":true},"prevItem":{"title":"How To Install and Configure SnowSQL","permalink":"/how-to-install-and-configure-snowsql"},"nextItem":{"title":"GDPR for Engineers - What is Personal Data?","permalink":"/gdpr-for-engineers-what-is-personal-data"}},"content":"GDPR was approved by EU parliament on April 14, 2016, went into effect May 25, 2018, and impacts any business handling any personal data of any EU resident.\\n\\nAt a high level, GDPR is a directive on the protection of personal data and can be scoped twofold. First, the law protects persons concerned by processing of personal data. Second, it enforces additional accountability on businesses involved in the processing of personal data.\\n\\nWhat does GDPR do exactly? Let\'s dive in.\\n\\n\x3c!--truncate--\x3e\\n\\n# GDPR increases territorial scope\\n\\nThe law applies to all companies processing personal data of subjects residing in the EU. Again, if your systems handle any PII of any person residing in the European Union, you are responsible to comply with any and all privacy regulation.\\n\\n### Are you a Boston-based business and serve EU customers when they visit as tourists every summer?\\n\\nYou need to comply.\\n\\n### Are you a US-based business with a European presence?\\n\\nYou need to comply.\\n\\n### Are you a European-based business with no international presence?\\n\\nYou need to comply.\\n\\n### Are you a European-based business with significant international presence?\\n\\nYou need to comply.\\n\\n# GDPR mandates explicit consent when tracking behavior\\n\\nImplicit consent, which is common in the United States, is not adequate according to GDPR.\\n\\n### Consent language must be clear, accessible, and intelligible.\\n\\nInstead of privacy policies being filled with often-intelligible legalese, GDPR mandates that privacy policies, cookie banners, or other forms of gathering tracking consent must use clear and accessible language which makes the purpose and scope easily understood.\\n\\n### Purpose of consent must be attached.\\n\\nVaguely requesting consent for \\"cookies\\" is no longer enough - GDPR states the purpose of the tracking be expressed. If your systems (like remarketing, advertising, or event tracking pixels) persist site visitor PII, you now have to provide the visitors with a reason for said tracking.\\n\\n### Tracking must be accepted (and is opt-out by default).\\n\\nIn the United States, tracking is typically opt-in by default with a notice effectively stating, \\"if you continue to use this site, you have implicitly allowed us to track you\\". This is not acceptable according to GDPR, as tracking must be opt-out by default and the site visitor must explicitly give their consent to be tracked.\\n\\n### It must be as easy to withdraw consent as it is to give it.\\n\\nSince post-GDPR tracking is opt-out by default and a user must give consent with full knowledge of the intended purpose, companies are naturally incentivized to make it as easy as possible to give consent. But wait, there\'s more! Once consent has been granted, it must be as easy for a user to revoke consent as it was to initially give it.\\n\\n\\n# GDPR provides specific rights for data subjects.\\n\\nNot entirely unlike the United States\' Bill of Rights or the United Nations\' Universal Declaration of Human Rights, GDPR establishes a notion of \\"basic digital rights\\" for data subjects. These rights are outlined as follows:\\n\\n### Breach notification\\n\\nAccording to GDPR, all individuals have a right to be notified if a company experiences a breach involving their personal data. This breach notification process is mandatory if the breach is likely to result in any risk to an individual\'s rights or freedoms. It must also be done within 72 hours of first awareness of said breach. If your company is a data processor that serves controllers, you must notify those controllers in the same manner.\\n\\n### Right to access\\n\\nData subjects can now obtain confirmation from a controller as to whether their personal data is being processed, where it is being processed, and what purpose the processing serves. They can also request the controller provide a copy of all personal data, free of charge, in a \\"common electronic format\\".\\n\\n### Right to be forgotten\\n\\nThis \\"basic digital right\\" really means one thing: upon request of an individual, a company must erase all personal data generated by or relating to that individual. The controller must cease dissemination of said data, and third parties must halt the processing of it.\\n\\n### Data portability\\n\\nLastly, a data subject can request all personal data concerning them and retains the full right to directly transfer the data to another controller. The data must be generated and transferred in a common format.\\n\\n# GDPR mandates privacy by design.\\n\\nA privacy-first approach to data and systems engineering is another major component of GDPR. The law is written rather vaguely (since it\'s very difficult to give overarching directives on how a system should be built) and states, \\"a controller should implement appropriate technical and organizational measures\\" for securing personal data. It also declares a controller should \\"hold and process only necessary data\\", and should \\"limit access to personal data to those doing the processing\\" of it.\\n\\nEnsuring your systems have been built in a way that enables your business to comply with the above is no small feat.\\n\\n# GDPR establishes data protection officers.\\n\\nIf your core business activities consist of processing operations which require regular, systematic monitoring of subjects on a larger scale, special categories of data, or data pertaining to criminal offenses, a data protection officer must be designated.\\n\\nThis officer should be appointed on the basis of professional qualities and expert knowledge of data protection law and practices, must be a staff member or external service provider, and must report directly to the highest level of management.\\n\\nThe data protection officer must be provided appropriate resources to carry out tasks and maintain expert knowledge and must not carry out tasks that could result in a potential conflict of interest.\\n\\nFinally, contact information of the data protection officer must be included in all relevant DPA\'s.\\n\\n# GDPR defines the enforcement of non-compliance.\\n\\nWhat happens when you don\'t comply with GDPR? The law makes it pretty clear: organizations can be fined up to \u20ac20 million or 4% of annual worldwide revenue for the prior year (whichever is greater). GDPR fines are tiered and apply to both controllers and processors.\\n\\nClouds are not exempt from enforcement.\\n\\n# In conclusion\\n\\nWhen it comes to processing and storing data, GDPR changes a lot of things. Properly complying with the regulation listed above is non-trivial, but ultimately gives individuals basic digital rights. As an engineer, it\'s incredibly important to understand the provisions listed and their implications on the systems you build.\\n\\nWould you be able to secure a breach, identify all affected parties, and communicate the magnitude within 72 hours?\\n\\nDo you have a good audit of all personal data moving through your systems (or third-party systems)?\\n\\nWould you be able to provide a data subject with a copy of all of their data if asked to do so?\\n\\nWould you be able to truly erase all personal data of an individual, if requested?\\n\\nIt\'s hard, but is by no means impossible.\\n\\n# Further reading\\n\\n\\nI\'ve found the following resources to be useful when learning about GDPR law (and the implications of it when building systems):\\n\\n- [https://eugdpr.org/](https://eugdpr.org/)\\n- [https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/data-protection/2018-reform-eu-data-protection-rules_en](https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/data-protection/2018-reform-eu-data-protection-rules_en)\\n- [https://ec.europa.eu/info/law/law-topic/data-protection_en](https://ec.europa.eu/info/law/law-topic/data-protection_en)"},{"id":"gdpr-for-engineers-what-is-personal-data","metadata":{"permalink":"/gdpr-for-engineers-what-is-personal-data","source":"@site/blog/2019-05-07-what-is-personal-data.md","title":"GDPR for Engineers - What is Personal Data?","description":"We all know that GDPR (also known as RGPD in France) has brought data policy into the spotlight for many technical organizations. As of May 25, 2018, if your systems (both automated and otherwise!) handle PII of individuals residing in the EU, you must comply with regulation. While this enforcement date makes the topic seem like old news, many US-based companies are unclear of the specifics and vastly underprepared to deal with the implications.","date":"2019-05-07T00:00:00.000Z","formattedDate":"May 7, 2019","tags":[{"label":"gdpr","permalink":"/tags/gdpr"},{"label":"engineers","permalink":"/tags/engineers"},{"label":"data privacy","permalink":"/tags/data-privacy"}],"readingTime":3.42,"truncated":true,"authors":[{"name":"Jake"}],"frontMatter":{"slug":"gdpr-for-engineers-what-is-personal-data","title":"GDPR for Engineers - What is Personal Data?","authors":{"name":"Jake"},"tags":["gdpr","engineers","data privacy"],"hide_table_of_contents":true},"prevItem":{"title":"GDPR for Engineers - What You Need to Know","permalink":"/gdpr-for-engineers-what-you-need-to-know"},"nextItem":{"title":"Client-side instrumentation for under $1 per month. No servers necessary.","permalink":"/client-side-instrumentation-for-under-one-dollar"}},"content":"We all know that GDPR (also known as RGPD in France) has brought data policy into the spotlight for many technical organizations. As of May 25, 2018, if your systems (both automated and otherwise!) handle PII of individuals residing in the EU, you must comply with regulation. While this enforcement date makes the topic seem like old news, many US-based companies are unclear of the specifics and vastly underprepared to deal with the implications.\\n\\nBefore diving too far into the deep end of implementation detail, one must first understand the basics. The only way to conform to this regulation (which many US-based companies still don\'t) is to thoroughly understand what data needs to be handled with care.\\n\\nSo... what is personal data?\\n\\n\x3c!--truncate--\x3e\\n\\n**At its core, personal data is any information that relates to an identified or identifiable living individual. It also pertains to different pieces of information that, when collected together, can lead to the identification of an individual.**\\n\\nBut, what does this mean exactly from a practical perspective? Some of the following may be obvious, but some may not be. Let\'s dive in...\\n\\n\\n# PII\\n\\n### Name + Surname is personal data.\\n\\nThe name \\"George Washington\\" is PII. If your application includes user registration functionality or your systems process and ship orders to a named individual, you\'re dealing with PII.\\n\\n### Home address is personal data.\\n\\nAgain, if you ship orders to a home address you\'re handling PII.\\n\\n### An email address, if it includes personal information like first/last name, is personal data.\\n\\ngeorgewashington1792@hotmail.com is PII. Since there\'s no guarantee incoming email addresses will be inherently free of PII, it is a good idea to consider all email addresses PII.\\n\\n### Any personal identification card number is personal data.\\n\\nBank account number, driver\'s license number, passport number, and social security number are all PII.\\n\\n### Location data (device information from a phone or laptop) is personal data.\\n\\nIncoming latitude and longitude data (which flows freely from mobile devices) is considered PII.\\n\\n### IP address is personal data.\\n\\nClient IP address is considered PII. If your systems log X-Forwarded-For or any form of the original client IP, your logs are filled with PII.\\n\\n### Various cookies can be personal data.\\n\\nIf your application stores any form of PII in cookies, these cookies are also PII (and the transitive property of equality has struck again).\\n\\n### Ad identifiers (android/ ios devices) are personal data.\\n\\nAndroid and iOS devices have device-specific advertising identifiers. These are considered PII.\\n\\n### Healthcare data is usually personal data.\\n\\nThis one goes without saying.\\n\\n### Pseudonymized data that can be used in conjunction with another source to re-identify is PII.\\n\\nIf you\'re pseudonymizing \\"email\\", but the output of the pseudonymization process can used alongside another source to tie back to the original individual, it\'s still PII.\\n\\n# NOT PII\\n\\n### Formerly-personal data that has been rendered completely anonymous is no longer personal data.\\n\\nIf a piece of PII has been rendered completely anonymous (typically through hashing it), it is no longer considered personal data. MD5-ing data like email addresses or first/last names is a perfectly acceptable method of eliminating the PII designation, especially when it is combined with a salt.\\n\\n### Company registration identifiers are not personal data.\\n\\nRegistration details used internally within a specific company to designate an individual are not considered PII.\\n\\n### Primary keys on your \\"users\\" table are not personal data.\\n\\nDatabase internals (like a primary key or unique user-specific `uuid` value) are not considered PII, and are an easy way to isolate PII without losing referential integrity.\\n\\n### Email addresses that do not include personal information are not personal data.\\n\\ntwinkylvr234@hotmail.com is (sadly) not considered PII. But again, it\'s a good ideal to treat all emails as PII.\\n\\n# In conclusion\\n\\nPII is everywhere! GDPR has drastically changed the requirements for data processing and storage of personal data, but without knowing what \\"personal details\\" are exactly it is very hard to comply with regulation. If your application collects, processes, or stores personal data of European residents, you\'re responsible for knowing and complying with the laws."},{"id":"client-side-instrumentation-for-under-one-dollar","metadata":{"permalink":"/client-side-instrumentation-for-under-one-dollar","source":"@site/blog/2018-11-20-client-side-instrumentation-under-one-dollar/index.md","title":"Client-side instrumentation for under $1 per month. No servers necessary.","description":"In a world where the importance of data is steadily increasing yet the cost of computing power is steadily decreasing, there are fewer and fewer excuses to not have control of your own data. To explore that point I instrumented this site as inexpensively as I possibly could, without sacrificing reliability or functionality. I have full control of all data that is generated, the instrumentation is highly customizable, the output is simple to use, and I don\'t have to be available at all hours to keep it working.","date":"2018-11-20T00:00:00.000Z","formattedDate":"November 20, 2018","tags":[{"label":"snowplow","permalink":"/tags/snowplow"},{"label":"serverless","permalink":"/tags/serverless"},{"label":"instrumentation","permalink":"/tags/instrumentation"}],"readingTime":8.965,"truncated":true,"authors":[{"name":"Jake"}],"frontMatter":{"slug":"client-side-instrumentation-for-under-one-dollar","title":"Client-side instrumentation for under $1 per month. No servers necessary.","authors":{"name":"Jake"},"tags":["snowplow","serverless","instrumentation"],"hide_table_of_contents":true},"prevItem":{"title":"GDPR for Engineers - What is Personal Data?","permalink":"/gdpr-for-engineers-what-is-personal-data"},"nextItem":{"title":"Why Your Company Should Own Its Own Data","permalink":"/why-your-company-should-own-its-own-data"}},"content":"In a world where the importance of data is steadily increasing yet the cost of computing power is steadily decreasing, there are fewer and fewer excuses to not have control of your own data. To explore that point I instrumented this site as inexpensively as I possibly could, without sacrificing reliability or functionality. I have full control of all data that is generated, the instrumentation is highly customizable, the output is simple to use, and I don\'t have to be available at all hours to keep it working.\\n\\nThe components utilized include Cloudfront, S3, Lambda, Athena, and the Snowplow Javascript tracker.\\n\\n**And It costs less than $1 per month.**\\n\\n\x3c!--truncate--\x3e\\n\\n### Before getting started.... what does the system need?\\n\\n- Easy javascript tracking integration, with no impact on site performance.\\n- Absolute minimization of resources for me to manage, pay for, and worry about.\\n- Rock-solid pipeline reliability.\\n\\nSo let\'s get started...\\n\\n# No servers.\\n\\nA good stream-based analytics infrastructure is a beautiful thing when it\'s done correctly, but there\'s always overhead involved. You\'ll need streams, you\'ll need collection servers sitting behind a load balancer/reverse proxy, you\'ll need machines solely responsible for enrichment, you\'ll need infrastructure for monitoring infrastructure, you\'ll need engineers who demand salaries, and.... you get the idea.\\n\\nFor this site I don\'t have any intention of modifying functionality or making decisions based on what visitors are doing \\"right now\\", so stream-based infrastructure just introduces overhead and is out of the question. Millisecond-level latency to the eventual data store (like in a stream-based approach) would be nice, but minute-level latency is perfectly ok.\\n\\n# No monitoring.\\n\\nBlasphemy! Or... maybe not. Instead of setting up lots of infrastructure that must be continually monitored, all instrumentation infrastructure leans heavily on AWS functionality. By taking advantage of various AWS built-ins (with a high level of reliability), I was able to skip a lot of overhead that would otherwise be required.\\n\\nCloudfront includes the ability to forward access logs to an S3 bucket.\\n\\nLambda triggers ensure at-least-once notification of a predefined event (like when a file lands in S3).\\n\\nS3 provides 99.999999999% durability and 99.99% availability on a yearly SLA.\\n\\n# No fees (almost).\\n\\nThe current cost of this system is fairly satisfying, but my favorite part is the fact that site traffic would have to increase significantly before cost increases beyond what I\'m willing to pay.\\n\\nCloudfront costs ~$0.085 per GB out to the internet, ~$0.020 per GB out to origin, and $0.0075 per 10,000 requests.\\n\\n**Total Cloudfront cost this month: $0.03**\\n\\nLambda is free up to 1M requests and 3.2M seconds of compute time, per month.\\n\\n**Total Lambda cost this month: $0.00**\\n\\nS3 costs $0.023 per GB, per month, $0.01 per 1,000 PUT/COPY/POST requests, and $0.001 per 1,000 GET/SELECT requests.\\n\\n**Total S3 cost this month: $0.27**\\n\\nAthena costs $5 per TB of data scanned.\\n\\n**Total Athena cost this month: $0.01**\\n\\n# Quick and easy data access.\\n\\nAs data volume grows, I need to be able to query all data quickly and effectively in a primarily-ad-hoc fashion. AWS Athena is effectively presto-as-a-service, and an external table backed by S3 was a straight-forward, inexpensive way forward.\\n\\nPutting a visualization layer like Quicksight on top of Athena is very simple.\\n\\n# How it is implemented.\\n\\n### Step #1 - AWS Cloudfront Distribution\\n\\nThe first step of setting up instrumentation requires creating two S3 buckets and a Cloudfront distribution. I\'ve automated the setup with Terraform, and the block of code for doing so (not including tfvars/etc) looks like the following:\\n\\n```\\nresource \\"aws_s3_bucket\\" \\"src\\" {\\n  bucket = \\"${var.env}-${var.owner}-${var.system_tag}-lt-src\\"\\n  acl    = \\"public-read\\"\\n  tags {\\n    Name   = \\"${var.env}-${var.owner}-${var.system_tag}-lt-src\\"\\n    Env    = \\"${var.env}\\"\\n    System = \\"${var.system_tag}\\"\\n    Owner  = \\"${var.owner}\\"\\n  }\\n}\\n\\nresource \\"aws_s3_bucket_object\\" \\"object\\" {\\n  bucket       = \\"${aws_s3_bucket.src.bucket}\\"\\n  key          = \\"i\\"\\n  source       = \\"${\\"${path.module}/files/i\\"}\\"\\n  etag         = \\"${md5(file(\\"${path.module}/files/i\\"))}\\"\\n  acl          = \\"public-read\\"\\n  content_type = \\"image/gif\\"\\n}\\n\\nresource \\"aws_s3_bucket\\" \\"logs\\" {\\n  bucket = \\"${var.env}-${var.owner}-${var.system_tag}-lt-logs\\"\\n  acl    = \\"private\\"\\n  tags {\\n    Name   = \\"${var.env}-${var.owner}-${var.system_tag}-lt-logs\\"\\n    Env    = \\"${var.env}\\"\\n    System = \\"${var.system_tag}\\"\\n    Owner  = \\"${var.owner}\\"\\n  }\\n}\\n\\nresource \\"aws_cloudfront_distribution\\" \\"log_distribution\\" {\\n  origin {\\n    domain_name = \\"${aws_s3_bucket.src.bucket_regional_domain_name}\\"\\n    origin_id   = \\"S3-${aws_s3_bucket.src.bucket}\\"\\n  }\\n  enabled         = true\\n  is_ipv6_enabled = true\\n  comment         = \\"Cloudfront distribution for snowplow tracking pixel\\"\\n  logging_config {\\n    include_cookies = true\\n    bucket          = \\"${aws_s3_bucket.logs.bucket_regional_domain_name}\\"\\n    prefix          = \\"RAW\\"\\n  }\\n  aliases = [\\"sp.${var.env}.${var.primary_domain}\\"]\\n  default_cache_behavior {\\n    allowed_methods  = [\\"GET\\", \\"HEAD\\"]\\n    cached_methods   = [\\"GET\\", \\"HEAD\\"]\\n    target_origin_id = \\"S3-${aws_s3_bucket.src.bucket}\\"\\n    forwarded_values {\\n      query_string = true\\n      cookies {\\n        forward = \\"all\\"\\n      }\\n    }\\n    viewer_protocol_policy = \\"allow-all\\"\\n    min_ttl                = 0\\n    default_ttl            = 3600\\n    max_ttl                = 86400\\n  }\\n  restrictions {\\n    geo_restriction {\\n      restriction_type = \\"whitelist\\"\\n      locations        = [\\"AF\\", \\"AX\\", \\"AL\\", \\"DZ\\", \\"AS\\", \\"AD\\", \\"AO\\", \\"AI\\", \\"AQ\\", \\"AG\\", \\"AR\\", \\"AM\\", \\"AW\\", \\"AU\\", \\"AT\\", \\"AZ\\", \\"BS\\", \\"BH\\", \\"BD\\", \\"BB\\", \\"BY\\", \\"BE\\", \\"BZ\\", \\"BJ\\", \\"BM\\", \\"BT\\", \\"BO\\", \\"BA\\", \\"BW\\", \\"BV\\", \\"BR\\", \\"IO\\", \\"BN\\", \\"BG\\", \\"BF\\", \\"BI\\", \\"CV\\", \\"KH\\", \\"CM\\", \\"CA\\", \\"KY\\", \\"CF\\", \\"TD\\", \\"CL\\", \\"CN\\", \\"CX\\", \\"CC\\", \\"CO\\", \\"KM\\", \\"CG\\", \\"CD\\", \\"CK\\", \\"CR\\", \\"CI\\", \\"HR\\", \\"CU\\", \\"CW\\", \\"CY\\", \\"CZ\\", \\"DK\\", \\"DJ\\", \\"DM\\", \\"DO\\", \\"EC\\", \\"EG\\", \\"SV\\", \\"GQ\\", \\"ER\\", \\"EE\\", \\"SZ\\", \\"ET\\", \\"FK\\", \\"FO\\", \\"FJ\\", \\"FI\\", \\"FR\\", \\"GF\\", \\"PF\\", \\"TF\\", \\"GA\\", \\"GM\\", \\"GE\\", \\"DE\\", \\"GH\\", \\"GI\\", \\"GR\\", \\"GL\\", \\"GD\\", \\"GP\\", \\"GU\\", \\"GT\\", \\"GG\\", \\"GN\\", \\"GW\\", \\"GY\\", \\"HT\\", \\"HM\\", \\"VA\\", \\"HN\\", \\"HK\\", \\"HU\\", \\"IS\\", \\"IN\\", \\"ID\\", \\"IR\\", \\"IQ\\", \\"IE\\", \\"IM\\", \\"IL\\", \\"IT\\", \\"JM\\", \\"JP\\", \\"JE\\", \\"JO\\", \\"KZ\\", \\"KE\\", \\"KI\\", \\"KP\\", \\"KR\\", \\"KW\\", \\"KG\\", \\"LA\\", \\"LV\\", \\"LB\\", \\"LS\\", \\"LR\\", \\"LY\\", \\"LI\\", \\"LT\\", \\"LU\\", \\"MO\\", \\"MK\\", \\"MG\\", \\"MW\\", \\"MY\\", \\"MV\\", \\"ML\\", \\"MT\\", \\"MH\\", \\"MQ\\", \\"MR\\", \\"MU\\", \\"YT\\", \\"MX\\", \\"FM\\", \\"MD\\", \\"MC\\", \\"MN\\", \\"ME\\", \\"MS\\", \\"MA\\", \\"MZ\\", \\"MM\\", \\"NA\\", \\"NR\\", \\"NP\\", \\"NL\\", \\"NC\\", \\"NZ\\", \\"NI\\", \\"NE\\", \\"NG\\", \\"NU\\", \\"NF\\", \\"MP\\", \\"NO\\", \\"OM\\", \\"PK\\", \\"PW\\", \\"PA\\", \\"PG\\", \\"PY\\", \\"PE\\", \\"PH\\", \\"PN\\", \\"PL\\", \\"PT\\", \\"PR\\", \\"QA\\", \\"RE\\", \\"RO\\", \\"RU\\", \\"RW\\", \\"BL\\", \\"KN\\", \\"LC\\", \\"MF\\", \\"PM\\", \\"VC\\", \\"WS\\", \\"SM\\", \\"ST\\", \\"SA\\", \\"SN\\", \\"RS\\", \\"SC\\", \\"SL\\", \\"SG\\", \\"SX\\", \\"SK\\", \\"SI\\", \\"SB\\", \\"SO\\", \\"ZA\\", \\"GS\\", \\"SS\\", \\"ES\\", \\"LK\\", \\"SD\\", \\"SR\\", \\"SJ\\", \\"SE\\", \\"CH\\", \\"SY\\", \\"TJ\\", \\"TH\\", \\"TL\\", \\"TG\\", \\"TK\\", \\"TO\\", \\"TT\\", \\"TN\\", \\"TR\\", \\"TM\\", \\"TC\\", \\"TV\\", \\"UG\\", \\"UA\\", \\"AE\\", \\"GB\\", \\"US\\", \\"UM\\", \\"UY\\", \\"UZ\\", \\"VU\\", \\"VE\\", \\"VN\\", \\"VG\\", \\"VI\\", \\"WF\\", \\"EH\\", \\"YE\\", \\"ZM\\", \\"ZW\\"]\\n    }\\n  }\\n\\n  tags {\\n    Name   = \\"${var.env}-${var.owner}-${var.system_tag}-log-dist\\"\\n    Env    = \\"${var.env}\\"\\n    System = \\"${var.system_tag}\\"\\n    Owner  = \\"${var.owner}\\"\\n  }\\n\\n  viewer_certificate {\\n    cloudfront_default_certificate = true\\n  }\\n}\\n```\\n\\nThis Cloudfront distribution serves a gif titled \\"i\\" from S3, which becomes the tracking endpoint. All access logs are then forwarded to another S3 bucket where they are subsequently processed.\\n\\n### Step #2 - Snowplow Javascript Tracker\\n\\nI am a huge fan of Snowplow. Integration is easy, functionality is rich, fault tolerance is built-in, and I know from experience that the system works very, very well.\\n\\nPage views, page pings, link clicks, and forms are tracked by default. I\'ve also added custom structured events for tracking specific actions that are otherwise uninstrumented.\\n\\nThe Javascript tracking code gets included in the ```<head>``` of every page, and looks like the following:\\n\\n```\\n<script type=\\"text/javascript\\">\\n  ;(function(p,l,o,w,i,n,g){if(!p[i]){p.GlobalSnowplowNamespace=p.GlobalSnowplowNamespace||[];\\n  p.GlobalSnowplowNamespace.push(i);p[i]=function(){(p[i].q=p[i].q||[]).push(arguments)\\n  };p[i].q=p[i].q||[];n=l.createElement(o);g=l.getElementsByTagName(o)[0];n.async=1;\\n  n.src=w;g.parentNode.insertBefore(n,g)}}(window,document,\\"script\\",\\"//d1fc8wv8zag5ca.cloudfront.net/2.6.2/sp.js\\",\\"snowplow\\"));\\n\\n\\n\\n\\n  window.snowplow(\'newTracker\', \'cf\', \'dolaqvbw76wrx.cloudfront.net\', {\\n    appId: \'site\',\\n    cookieDomain: \'bostata.com\',\\n  });\\n  window.snowplow(\'enableActivityTracking\', 1, 5);\\n  window.snowplow(\'trackPageView\');\\n  window.snowplow(\'enableLinkClickTracking\');\\n  window.snowplow(\'enableFormTracking\');\\n<\/script>\\n```\\n\\n### Step #3 - AWS Lambda\\n\\nThe next aspect of this infrastructure is a Lambda function that listens to the Cloudfront distribution log bucket for any ObjectCreated event, processes all incoming access logs, appends rich context to them (geoip, useragent parser, query parameter parsing, etc), formats these records in a consistent manner to the Snowplow canonical event model, and persists formatted records back to the log bucket as gzipped json files.\\n\\nUp to 1000 concurrent instances of this function can be running at any time, and each instance is limited to 128mb of memory.\\n\\n### Step #4 - AWS Athena\\n\\nIt\'s fun to watch data pile up in S3, but the value of said data is very low if it\'s difficult to access. Athena allows you to query S3 buckets quickly and easily via external tables... and the price is right. Creating a table for the Snowplow canonical event model is pretty simple:\\n\\n```\\nCREATE EXTERNAL TABLE default.events\\n(\\n  app_id string,\\n  platform string,\\n  etl_tstamp timestamp,\\n  collector_tstamp timestamp,\\n  dvce_created_tstamp timestamp,\\n  event string,\\n  event_id string,\\n  txn_id string,\\n  name_tracker string,\\n  v_tracker string,\\n  v_collector string,\\n  v_etl string,\\n  user_id string,\\n  user_ipaddress string,\\n  user_fingerprint string,\\n  domain_userid string,\\n  domain_sessionidx string,\\n  network_userid string,\\n  geo_country string,\\n  geo_region string,\\n  geo_city string,\\n  geo_zipcode string,\\n  geo_latitude string,\\n  geo_longitude string,\\n  geo_region_name string,\\n  ip_isp string,\\n  ip_organization string,\\n  ip_domain string,\\n  ip_netspeed string,\\n  page_url string,\\n  page_title string,\\n  page_referrer string,\\n  page_urlscheme string,\\n  page_urlhost string,\\n  page_urlport string,\\n  page_urlpath string,\\n  page_urlquery string,\\n  page_urlfragment string,\\n  refr_urlscheme string,\\n  refr_urlhost string,\\n  refr_urlport string,\\n  refr_urlpath string,\\n  refr_urlquery string,\\n  refr_urlfragment string,\\n  refr_medium string,\\n  refr_source string,\\n  refr_term string,\\n  mkt_medium string,\\n  mkt_source string,\\n  mkt_term string,\\n  mkt_content string,\\n  mkt_campaign string,\\n  contexts string,\\n  se_category string,\\n  se_action string,\\n  se_label string,\\n  se_property string,\\n  se_value string,\\n  unstruct_event string,\\n  tr_orderid string,\\n  tr_affiliation string,\\n  tr_total string,\\n  tr_tax string,\\n  tr_shipping string,\\n  tr_city string,\\n  tr_state string,\\n  tr_country string,\\n  ti_orderid string,\\n  ti_sku string,\\n  ti_name string,\\n  ti_category string,\\n  ti_price string,\\n  ti_quantity string,\\n  pp_xoffset_min string,\\n  pp_xoffset_max string,\\n  pp_yoffset_min string,\\n  pp_yoffset_max string,\\n  useragent string,\\n  br_name string,\\n  br_family string,\\n  br_version string,\\n  br_type string,\\n  br_renderengine string,\\n  br_lang string,\\n  br_features_pdf string,\\n  br_features_flash string,\\n  br_features_java string,\\n  br_features_director string,\\n  br_features_quicktime string,\\n  br_features_realplayer string,\\n  br_features_windowsmedia string,\\n  br_features_gears string,\\n  br_features_silverlight string,\\n  br_cookies string,\\n  br_colordepth string,\\n  br_viewwidth string,\\n  br_viewheight string,\\n  os_name string,\\n  os_family string,\\n  os_manufacturer string,\\n  os_timezone string,\\n  dvce_type string,\\n  dvce_ismobile string,\\n  dvce_screenwidth string,\\n  dvce_screenheight string,\\n  doc_charset string,\\n  doc_width string,\\n  doc_height string,\\n  tr_currency string,\\n  tr_total_base string,\\n  tr_tax_base string,\\n  tr_shipping_base string,\\n  ti_currency string,\\n  ti_price_base string,\\n  base_currency string,\\n  geo_timezone string,\\n  mkt_clickid string,\\n  mkt_network string,\\n  etl_tags string,\\n  dvce_sent_tstamp timestamp,\\n  refr_domain_userid string,\\n  refr_dvce_tstamp timestamp,\\n  derived_contexts string,\\n  domain_sessionid string,\\n  derived_tstamp timestamp,\\n  event_vendor string,\\n  event_name string,\\n  event_format string,\\n  event_version string,\\n  event_fingerprint string,\\n  file_name string\\n)\\nROW FORMAT  serde \'org.apache.hive.hcatalog.data.JsonSerDe\'\\nLOCATION    \'s3://prod-logs-bucket/PROCESSED\';\\n```\\n\\n# See it at work\\n\\nSince it\'s always fun to see a system working in the wild, let\'s watch exactly how this instrumentation works. I\'m using the developer tools on Chrome to inspect browser network traffic.\\n\\nEach time a page is loaded, the browser requests a sp.js file that\'s cached in Cloudfront as seen here:\\n\\n![spjs](spjs.png)\\n\\nImmediately following that, a page view event gets fired to the Cloudfront distribution that was set up above:\\n\\n![pageview](02_pageview.png)\\n\\nA page ping event fires to the Cloudfront distribution every five seconds, and indicates that I\'m still actively reading or scrolling:\\n\\n![pageping](03_pageping.png)\\n\\nWith all of this wonderful tracking, the Lambda function will start going to work:\\n\\n![lambda](04_lambda.png)\\n\\nAnd by the time you\'ve reached this point, the data will be readily accessible in S3 and queryable via Athena:\\n\\n![athena](athena.png)\\n\\n\\n# In conclusion\\n\\nOverall, I\'d say this exploration has been a big win. Client-side site instrumentation typically requires significant setup and maintenance overhead, but this methodology is the exact opposite. It\'s cheap. It\'s fast. It\'s quick to set up. It\'s reliable. It\'s flexible. And it just works.\\n\\nThe price of system operation will continue to be minimal, the data stays close-to-home, and I don\'t have to think about data pipeline issues.\\n\\nAnd all for under $1 per month."},{"id":"why-your-company-should-own-its-own-data","metadata":{"permalink":"/why-your-company-should-own-its-own-data","source":"@site/blog/2018-10-07-why-you-should-own-your-data.md","title":"Why Your Company Should Own Its Own Data","description":"When considering software and related infrastructure, the business of today is caught in a never-ending cycle of \\"build vs. buy\\". Many third-party companies solve serious challenges such as managing sales pipelines, accounting automation, payment processing, and internal communication. These alternatives to \\"building it yourself\\" empower companies to operate faster or more efficiently, and overall benefit to the customer is often net-positive. When considering various alternatives, there is one critical component of your business that you should strongly reconsider leaving in the hands of third parties, however: your data and supporting data infrastructure.","date":"2018-10-07T00:00:00.000Z","formattedDate":"October 7, 2018","tags":[{"label":"data","permalink":"/tags/data"},{"label":"ownership","permalink":"/tags/ownership"}],"readingTime":4.87,"truncated":true,"authors":[{"name":"Jake"}],"frontMatter":{"slug":"why-your-company-should-own-its-own-data","title":"Why Your Company Should Own Its Own Data","authors":{"name":"Jake"},"tags":["data","ownership"],"hide_table_of_contents":true},"prevItem":{"title":"Client-side instrumentation for under $1 per month. No servers necessary.","permalink":"/client-side-instrumentation-for-under-one-dollar"},"nextItem":{"title":"Data Pipeline Design Considerations","permalink":"/data-pipeline-design-considerations"}},"content":"When considering software and related infrastructure, the business of today is caught in a never-ending cycle of \\"build vs. buy\\". Many third-party companies solve serious challenges such as managing sales pipelines, accounting automation, payment processing, and internal communication. These alternatives to \\"building it yourself\\" empower companies to operate faster or more efficiently, and overall benefit to the customer is often net-positive. When considering various alternatives, there is one critical component of your business that you should strongly reconsider leaving in the hands of third parties, however: your data and supporting data infrastructure.\\n\\n\x3c!--truncate--\x3e\\n\\nThe most progressive companies in the world, of all industries and sizes, have one thing in common: an obsession with the collection and in-house ownership of data. Why invest in your own data infrastructure? It adds long-term value to your organization, has positive financial implications, and gives your company competitive advantages that are achievable in no other way.\\n\\n# Owning your data adds to your company\'s long-term value.\\n\\nImagine you\'re an investor, evaluating two functionally-equivalent companies. Both companies have similar technology, a large customer base, provide real value to the world, and have been in business for about five years. Company 1 has four years\' worth of site traffic and customer behavior, application performance metrics, clean financial data, git (code change) history, and a number of external datasets relevant to its core competencies. Company 2 simply has its cofounders and a small subset of the engineering it has employed along the way.\\n\\n**Which one would you invest in?**\\n\\n*Probably not the one with minimal data to back their claims up.*\\n\\nHaving full ownership of your own data makes tracking unsampled site performance over time easy. With a product like Google Analytics, on the other hand, that is impossible to do unless you are held hostage at an annual cost of over $150,000. Want to definitively prove business growth or product efficacy? Want to prove your systems have become more efficient or technical competence has increased throughout your company\'s existence? Instrument your systems and warehouse the data.\\n\\nIt is only through in-house ownership of your data that this is possible.\\n\\n# Owning your data has large financial implications- both now and in the future.\\n\\nLet\'s consider a widely-used system for site instrumentation and analtics previously mentioned: Google Analytics. If you want a \\"pure\\", unsampled view of your site traffic you will need to become a Google Analytics Premium (now branded \\"360\\") customer at a cost of over $150,000 annually. What happens when you become a paying customer, only to decide five years down the road that you need to cut costs and downgrade the service? All your historical data will be sampled, and you will be left with only a rough estimate of site traffic over time. You certainly wouldn\'t want to lose insight you once had, and are therefore held hostage by your own data.\\n\\nLet\'s consider another commonly-used platform for application instrumentation: Mixpanel. Mixpanel is incredibly easy to set up, straight-forward to add custom instrumentation, and initially inexpensive. But as your site scales or you want to instrument more and more components, the cost of the service becomes prohibitively expensive. Whether or not your revenue scales relative to site traffic, the cost to you certainly will. At a rate that is typically higher than your business itself is scaling. Again, you don\'t want to lose historical context of your business by downgrading instrumentation, so you become held hostage by the very data your application generated.\\n\\n# Owning your data gives you competitive advantages that are achievable in no other way.\\n\\nWhen your site scales yet the associated data collection systems are owned by another company (and rented by yours), who has the advantage? The answer is simple: not you!\\n\\nWhat happens when you stop paying a third-party service?\\n\\nYou lose access, and they keep your data. If they\'re lucky, you forgot about the tracking code placed on your site, and continue to ship them data for an extended period of time.\\n\\nWhat happens when Google wants to launch a product that directly competes with yours?\\n\\nThey have years of your site traffic data, and probably your closest competitors\' data as well.\\n\\nWhat happens when the data your systems or customers produced has insight that can be monetized, or activity that can be aggregated, enriched, and sold to a partner?\\n\\nYour company won\'t be seeing any of those financial benefits, sorry!\\n\\nConversely...\\n\\n# If you own your data, you have full control.\\n\\nWant to track activity at a finer granularity than you can with an existing tool? You can do that, because it\'s yours!\\n\\nWant to ensure you comply with GDPR?\\n\\nIt\'s much easier if you control the collection and persistence of personal data, and can simply issue a `DELETE FROM some_table WHERE id = 10;` sql statement!\\n\\nWant to persist terabytes of data in a 99.999999999% durable location that is accessible at any time? You can do so with AWS S3 at a whopping price of $0.023 per GB, per month. Have more data than that, and want to roll it into long-term cold storage? AWS Glacier exists for exactly that reason, and is priced at a convenient $0.004 per GB, per month. Thanks to Moore\'s law, data storage gets less expensive over time, unlike any third-party data service provider on the market.\\n\\n# Conclusion\\n\\nIn order to build the most value possible for your organization, realize maximum financial benefits, and retain various competitive advantages, you should own your data. While many third-party data services will be easy to set up initially, your business will probably find itself with maximum expenditures, minimum flexibility, lacking ownership of years\' of data it has created, and a lot of hassle long term. In order to maintain maximum competitive advantage, future flexibility, and overall cost efficiencies, you must own and manage your own data."},{"id":"data-pipeline-design-considerations","metadata":{"permalink":"/data-pipeline-design-considerations","source":"@site/blog/2018-02-09-data-pipeline-design-considerations/index.md","title":"Data Pipeline Design Considerations","description":"There are many factors to consider when designing data pipelines, which include disparate data sources, dependency management, interprocess monitoring, quality control, maintainability, and timeliness. Toolset choices for each step are incredibly important, and early decisions have tremendous implications on future successes. The following post is meant to be a reference to ask the right questions from the start of the design process, instead of halfway through. In terms of the V-Model of systems engineering, it is intended to fall between the \u201chigh level design\u201d and \u201cdetailed design\u201d steps:","date":"2018-02-09T00:00:00.000Z","formattedDate":"February 9, 2018","tags":[{"label":"data","permalink":"/tags/data"},{"label":"pipeline","permalink":"/tags/pipeline"},{"label":"design","permalink":"/tags/design"}],"readingTime":13.78,"truncated":true,"authors":[{"name":"Jake"}],"frontMatter":{"slug":"data-pipeline-design-considerations","title":"Data Pipeline Design Considerations","authors":{"name":"Jake"},"tags":["data","pipeline","design"],"hide_table_of_contents":true},"prevItem":{"title":"Why Your Company Should Own Its Own Data","permalink":"/why-your-company-should-own-its-own-data"}},"content":"There are many factors to consider when designing data pipelines, which include disparate data sources, dependency management, interprocess monitoring, quality control, maintainability, and timeliness. Toolset choices for each step are incredibly important, and early decisions have tremendous implications on future successes. The following post is meant to be a reference to ask the right questions from the start of the design process, instead of halfway through. In terms of the V-Model of systems engineering, it is intended to fall between the \u201chigh level design\u201d and \u201cdetailed design\u201d steps:\\n\\n\x3c!--truncate--\x3e\\n\\n![v-curve](v-curve.png)\\n\\n## Preliminary Considerations\\n\\nBefore selecting toolsets, and certainly before writing any code, there are many subsystem factors and expectations to be taken into account. When designing production-facing data pipelines, you will need to take these factors into consideration.\\n\\n# Data: Origin, Type, and Timeliness\\n\\n### Origin\\n\\nFirst and foremost, the origin of the data in question must be well understood, and that understanding must be shared across engineers to minimize downstream inconsistencies. Assumptions concerning data structure and interpretation are very hard to work around once they are baked into reports and/or managerial decisions, so it\u2019s incredibly important to get this step right.\\n\\nSecondly, an investigation into how to get data from the production application must be performed. Can application data be queried/ exported from the production database, in bulk, without detrimentally affecting the user experience? Are you sure about that? Spoiler alert: you\'ll probably want to set up replication on the production database before the export process is hardened.\\n\\n### Type\\n\\nThe type of data involved is another important aspect of system design, and data typically falls into one of two categories: event-based and entity data. Event-based data is denormalized, and is used to describe actions over time, while entity data is normalized (in a relational db, that is) and describes the state of an entity at the current point in time. In Ralph Kimball\u2019s data warehousing terminology, event-based data corresponds to facts while entity data corresponds to dimensions.\\n\\nAt first glance, event-based data lends itself to incremental ingestion via high-water marks, while entity data lends itself to bulk ingestion or change data capture. If the normalized data model includes a modified_at (or equivalent) column on entity tables, and it is trustworthy, various entity data can also be ingested incrementally to relieve unnecessary load. Ideally, data should always be incrementally ingested and processed, but reality says that is not always an option.\\n\\n### Timeliness\\n\\nHow quickly must data be gathered from the production system, and how quickly must it be processed? Data pipelining methodologies will vary widely depending on the desired speed of data ingestion and processing, so this is a very important question to answer prior to building the system. Ideally, event-based data should be ingested almost instantaneously to when it is generated, while entity data can either be ingested incrementally (ideally) or in bulk. If all data ingestion processes are incremental, making the process faster is simply just a matter of running the particular job more often. Therefore, that should be the goal.\\n\\nKafka is a very good option for realtime website activity tracking as it was created by Linkedin to do exactly that. It offers the ability for messages to be replayed, incorporates extensive fault-tolerance, can be partitioned, etc. RabbitMQ and Snowplow are other very suitable options, and solve similar problems in slightly different ways.\\n\\nFor pulling data in bulk from various production systems, toolset choices vary widely, depending on what technologies are implemented at the source. In Postgresql, these choices include `COPY (some_query) TO STDOUT WITH CSV HEADER`, a dblink from one database to another, streaming replication via the write-ahead log, or using a `pg_dump --table sometable --no-privileges | some_file.sql` script. Redshift\'s functionality is very similar, but the system uses S3 as an intermediary to `UNLOAD (\'some_query\') TO \'s3://bucket\'`.\\n\\n# Storage Mechanisms\\n\\nWhen it comes to choosing a storage mechanism, the largest factors to be considered include the volume of data and the query-ability of said data (if \\"query-ability\\" is indeed a word). If a limited amount of volume is expected, or data is pre-aggregated elsewhere, many storage options will suffice. If incoming data is to be collected in sufficiently large volume or if the storage mechanism must allow for downstream exploratory querying, storage options decrease significantly. If queries are defined beforehand and the volume of data is the limiting factor, Hadoop is a solid alternative. If a high volume of data is to be collected but will be queried in an exploratory way, Redshift is a better alternative. The parity to older versions of Postgres (8.0.2) and the fact that the surface looks/ feels like a regular Postgres database make it very easy to learn and utilize.\\n\\n### Questions to ask when choosing a storage mechanism\\n\\n- What level of volume is expected? 200M rows in a single table makes Postgres crawl, especially if it isn\u2019t partitioned. If it is partitioned, queries must be altered accordingly to avoid scanning each partition.\\n- Will data be exploratorily queried, or are queries defined already and will be semi-static in the future? Tool options and distribution/ sorting strategies will need to be altered accordingly.\\n- What level of maintenance do you wish to perform? Ideally, engineering resources will go to setting up the system, and future maintenance will be minimal and/or only performed as absolutely necessary.\\n- Will dashboarding/ analysis tools be pointed at the raw data, or will data be aggregated and moved elsewhere?\\n- Who will be accessing the data? A system like Hadoop is not ideal for junior analysts with limited SQL/bash knowledge.\\n\\n# Language Selection\\n\\nFor the sake of developmental speed, maintainability, and parity with various pipelining tools, Python is a very solid language selection. The key component to this is consistency across the system - especially when the team is small. Usually the \u201cPython is slow\u201d argument can be overcome by Python\u2019s multiprocessing or concurrent.futures modules, or by only processing as much as is absolutely necessary. Or by waiting for Intel to optimize for Python. But that\'s a different story.\\n\\nAlmost every single data pipelining tool has a Python client, and the language allows for future flexibility while maintaining speed of initial development. It also allows for rapid onboarding of new developers, efficient testing with mock, remote bash execution/deployment with fabric, and much more. Companies that continually process enormous amounts of data use Python extensively, and there\u2019s a reason they have selected it as language of choice. Lastly, Airbnb\u2019s Airflow and Spotify\u2019s Luigi are both conveniently written in Python.\\n\\n# ETL Dependency Management\\n\\nDependency management is easy to conceptualize, but a little more difficult to manage across multiple worker nodes or *cough* a Docker Swarm. This is where tools like Luigi, Airflow, and even Jenkins for remote execution scheduling come into play.\\n\\n### Time-based scheduling\\n\\nIn short, pipeline jobs should never be executed via time-based scheduling. If a one-hour job that is scheduled at 8am fails, all downstream jobs should be aware of that failure and downstream execution should be modified accordingly. If the job starts taking longer than one hour due to increased volume or processing requirements, downstream jobs should be aware and only execute after the upstream job has successfully run. Time-based scheduling cannot efficiently handle either condition, and requires engineers continuously modify the schedule. This wastes time, and is completely unnecessary.\\n\\n### Single vs. multiple dependencies\\n\\nIf an ETL job only has one upstream dependency, Jenkins is a perfectly suitable tool for linking jobs together. It is quite straight-forward to set up, allows for remote execution on multiple nodes, and is simply a matter of passing the necessary bash script to the job configuration. It also allows for one job to kick off multiple downstream tasks after execution (ie, \u201cload the data you just aggregated to a foreign database, and let the world know it\u2019s happening\u201d).\\n\\nIf an ETL job has multiple upstream dependencies, Jenkins becomes pretty clumsy. It is certainly possible to do so, but it\u2019s not exactly pretty. Luigi and Airflow shine in this respect, because both are built to handle many upstream dependencies for a single job (DAG style).\\n\\n### Visualizing Dependencies\\n\\nJenkins doesn\u2019t even try to visualize the acyclic graph of nested dependencies, while Luigi and Airflow both do. Visualizing the relationships between interconnected jobs is a huge time saver, and allows for the system to grow easily without overloading the ones who built it (and therefore slowing down future development).\\n\\n### Separation of Concerns\\n\\nJenkins is exceptional at job scheduling and handling a limited set of single dependencies, and that is all. Luigi is extremely good at multiple dependency handling and visualization, but it doesn\u2019t even attempt to handle scheduling execution of the initial job in the acyclic graph. Airflow tries to do everything including job duration monitoring, plotting job execution overlap via Gantt charts, scheduling, and dependency management. In my opinion (I warned you I was opinionated), job durations and overlap are ideally tracked and handled elsewhere, alongside other pipeline instrumentation.\\n\\n### Balancing dependency management with job overload\\n\\nIf a job dependency tool is used, every minuscule item of the ETL process should be not wrapped in a task. For example, if ten tables are to be exported from a remote database, and all must be exported before downstream tasks run, there should not be an import job for each of the ten tables. There should be one job that imports all the designated tables. This allows for the dependency graph to remain clean, code to be well-written and easily maintainable, etc.\\n\\n# Fault Tolerance\\n\\n### Persistence in message-based ingestion\\n\\nIf incoming event data is message-based, a key aspect of system design centers around the inability to lose messages in transit, regardless of what point the ingestion system is in. Message queues with delivery guarantees are very useful for doing this, since a consumer process can crash and burn without losing data and without bringing down the message producer. Alternatively, the producer process can crash and burn, and the consumer will see nothing but the fact that no new messages have come in. Messages in transit should always be persisted to disk (if space and time allows) so that if the broker/queue goes down, it can be brought back up without losing data.\\n\\nIf done right, this method of data ingestion is extremely fault-tolerant and scalable. There are some key differences between various pub/sub systems (persistence, replayability, distributed commit log vs. queue), but that is a separate conversation.\\n\\n### Stop-on-error in bulk processing\\n\\nFault tolerance is a key aspect of any data pipeline, and downstream tasks should always be aware of upstream tasks failing. Data pipelining is one place where exceptions should not always be handled in code, and engineers should know about any error in the system immediately. The main place exceptions should be handled is when retrying a task for a designated period of time (or number of retries/ exponential back-off). This is so that downstream jobs don\u2019t run and mistakenly cause additional harm to data quality. The system should stop immediately when a fault is detected if downstream jobs depend on it.\\n\\n### Idempotence and replayability\\n\\nBulk pipeline jobs should always be created so that they are able to be re-run immediately in case of failure, and entirely idempotent. No matter how many times a particular job is run, it should always produce the same output with a given input, and should not persist duplicate data to the destination. Want math?\\n\\n```f(f(x)) = f(x)```\\n\\n# Architectural separation of concerns\\n\\nIn short, a production web application should never be dependent on a reporting database and/or data warehouse. This creates an unnecessary dependency which is inflexible for maintenance and increases the level of risk.\\n\\n# Pipeline Monitoring\\n\\nAs with any system, individual steps should be extensively instrumented and monitored. If they are not, it is nearly impossible to eliminate personal opinion and accurately determine the facts of system operation. A good starting point is to measure the time which a particular job started, stopped, total runtime, state of completion, and any pertinent error messages.\\n\\n### Start and stop time\\n\\nThis one is pretty straight-forward, as the difference is used to calculate runtime. Logging start time of jobs is very useful on its own, however, since it adds a linear aspect to job execution (even if the jobs are executed in parallel). When debugging a step of the pipeline, it\u2019s very helpful to see when the jobs were executed, in order by which they were kicked off. It also allows for easy visualization if need be.\\n\\n### Runtime\\n\\nThis metric is also obvious yet incredibly powerful for determining the current state of the world, proactive system improvements, and early warning signs for future problems. The following insight is realized, simply by measuring runtime:\\n\\n- Determination of system bottlenecks\\n- Early-warning signs for jobs trending upwards, relative to other jobs\\n- Statistically-benchmarked improvements\\n- Determination of the effects on a particular system from external variables (server, network, etc)\\n- Job completion and error logging\\n\\nPipeline instrumentation not only allows for efficient debugging, early warning signs, and proactive improvements to the system, but it also allows the engineers to see progress over time. It also can be very easily used to expose various system statistics to outside parties, so that system scale can be easily and effectively communicated.\\n\\n# Pipeline Monitoring Tools\\n\\nMonitoring each portion of the pipeline, as well as aggregate statistics, can be done in a variety of ways. Graphite was built to handle this type of data, but even a table that stores system metrics is much better than nothing all all. The combination of instrumentation decorators (in Python at least) and various signals being sent to Graphite is ideal. InfluxDB is another way to build instrumentation directly into your data pipelines.\\n\\nFor system instrumentation visualizations, I highly recommend Grafana.\\n\\n# Accessibility and Visualization\\n\\n### Tools\\n\\nThere are many tools out there for accessing and visualizing data. For data exploration and team collaboration, tools like Wagon are great. For dashboarding and sharing data, Periscope is a good choice. When selecting a tool, it is ideal if the code involved is not proprietary to that particular tool. That is, the entire company shouldn\u2019t be bound to a tool, simply because it uses a sql variant that is too painful to rewrite. Another key consideration is the ease of use for non-technical or semi-technical people. In order to proliferate a data-centric mindset across the organization, the tool must be relatively straight-forward to use and build upon. Also, it doesn\u2019t really make sense to build out a proprietary visualization tool when time-series or bar charts powered by sql will suffice.\\n\\n### Database role topology\\n\\nWhen analysts and other dashboarding tools are allowed access to the same database as your various ETL tools, it is of utmost importance to grant them the minimum permissions necessary to perform the designated job function. In short, an analyst should never be able to inadvertently UPDATE event records coming from a production system, and a dashboarding tool should never be allowed the privileges necessary to issue a DROP TABLE statement. A flexible role topology allows the freedom for each of these groups to do what they need to do, but no more.\\n\\n# System constraints\\n\\nThere are many constraints to consider, and pipeline creation largely depends on data sources, deployment environment, associated networks, etc. If the source data/ production application is all on AWS, it doesn\u2019t make sense to spin up a physical server in the office for the pipeline. AWS offers plenty of tools for moving data within the system itself (as well as the cost implications when keeping AWS-generated data inside AWS). Alternatively, if the application lives on a VM, hosted in the office, it makes sense to spin up another VM on the same subnet for pipelining. Since constraints are entirely dependent on the existing system in which to gather data from, it would simply be impossible to cover all them in a simple paragraph. That\'s where a little human creativity goes a long way.\\n\\n# Conclusion\\n\\nIn conclusion, there are a plethora of options to consider when building out a data pipelining system. There are many pros and cons to each alternative, and this document outlines some of the major factors involved. I hope it helps, and at the very least, provokes questions and thoughtful system design. If thought through from the start, many system inefficiencies can be avoided, and the power associated with efficient, reliable data collection can rapidly come to fruition."}]}')}}]);
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://your-docusaurus-test-site.com/</id>
    <title>Bostata Blog</title>
    <updated>2021-12-19T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://your-docusaurus-test-site.com/"/>
    <subtitle>Bostata Blog</subtitle>
    <icon>https://your-docusaurus-test-site.com/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Staying Fresh with Freshness Tables]]></title>
        <id>staying-fresh-with-freshness-tables</id>
        <link href="https://your-docusaurus-test-site.com/staying-fresh-with-freshness-tables"/>
        <updated>2021-12-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Without freshness, why should anyone trust you or your systems? Why should they pay your company money or take your recommendations and "science" seriously? Why should they spend their time building on your systems or data warehouse?]]></summary>
        <content type="html"><![CDATA[<p>Without freshness, why should anyone trust you or your systems? Why should they pay your company money or take your recommendations and "science" seriously? Why should they spend their time building on your systems or data warehouse?</p><h1>But Freshness is Really Hard.</h1><p>Data warehouses store data... of different shapes and sizes.... sourced from different places... at different frequencies... and modeled in different ways. It's the data team's job to make that process transparent, reliable, and understandable.</p><p>As the use of data ramps in an organization (often rapidly), conversations tend to go from the initial 🤯🙌😮 to "how quickly is my data loaded?".</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="all-too-familiar-questions-look-something-like-this">All-too-familiar questions look something like this:<a class="hash-link" href="#all-too-familiar-questions-look-something-like-this" title="Direct link to heading">​</a></h3><p><strong>How often do we load our Postgres instances?</strong></p><p>"Every fifteen minutes."</p><p><strong>How about logs?</strong></p><p>"Daily."</p><p><strong>We have an event stream right?</strong></p><p>"Of course! Events land in Snowflake every three minutes."</p><p><strong>How about the Salesforce and Hubspot integrations?</strong></p><p>"Hubspot is loaded hourly. We ran into API limits with Salesforce so that's loaded every few hours. Less often if we exceed quotas and the retries kick in."</p><p><strong>Did we ever start loading Greenhouse? I want to dig into hiring funnels.</strong></p><p>"Sure did. Applications, candidates, stages, scorecards are all loaded twice per day. Rejection reasons are once per day."</p><p><strong>What about our client data feeds?</strong></p><p>"We grab them as often as we can, but most land on Thursday or Friday. And they never come in on the weekend. Manually-curated ones always go dark in December."</p><p><strong>This is great. How about our DBT models?</strong></p><p>"Table materializations are run twice per day. Incremental models run every hour. Our views pass-through to the source tables, so they are as fresh as the source tables."</p><p>"But we manually refresh the materialized views with Airflow every thirty minutes..."</p><p>"And we are playing with Materialize on Kafka, so that's usually within a few hundred ms..."</p><p>"And there are couple Snowflake tasks..."</p><p><em>....and the list goes on.</em></p><p>This conversation quickly grows as teams, data sources, technologies, data models, and stakeholder value grows. Engineers who were once thrilled to build new systems quickly become burdened by all the freshness. Managers who were once able to recruit and onboard team members or advance initiatives become the context bottleneck. "Ms. Manager, is this table fresh? How about this one? How often can I expect data to be here?"</p><h1>Measuring Freshness with Record-Level Metadata is OK.</h1><p>Once upon a time database people kept track of record-level metadata with createdAt, updatedAt, and deletedAt columns. And fun stuff like database triggers, functions, audit tables, chewing on the WAL, whatever.</p><p>This is all dandy! But only for the source database.</p><p>So data warehouse people added "warehouse" or "system" record-level metadata columns: "_updated_at", "dw_updated_at", "_loaded_at", "fivetran_synced" and the like. Sprinkle in some column defaults and you're well on your way.</p><p><a href="https://www.stitchdata.com/docs/replication/loading/system-tables-columns" target="_blank" rel="noopener noreferrer">Stitch</a> and <a href="https://fivetran.com/docs/getting-started/system-columns-and-tables" target="_blank" rel="noopener noreferrer">Fivetran</a> do this. It works! Querying "select max(fivetran_synced)" for all tables lets you quickly see the last time they were loaded. <strong>Fresh.</strong></p><p>DBT uses <a href="https://docs.getdbt.com/reference/resource-properties/freshness" target="_blank" rel="noopener noreferrer">Freshness Checks</a> to "select max(loaded_at)" sources. And provides a handy Source Freshness UI to give a visual representation of source status. <strong>Definitely fresh.</strong></p><p><strong><em>But there are some downsides to all this freshness. Especially in columnar databases.</em></strong></p><p>As tables grow in size a "select max(loaded_at)" is super wasteful. These tables are rarely partitioned or clustered on load-metadata columns. And if tables are initially clustered by "loaded at", they probably won't be for very long. Freshness checks will therefore be less efficient (and slower) as the table gets larger, and you will spend more and more money trying to figure out how fresh things are. With limited success. <strong>Not fresh.</strong></p><p>If you're using BigQuery, each freshness check will use a job slot. Which is ok, until you scale. Or have more people checking dashboards. Or run things more frequently. A freshness check that accelerates your systems bouncing off BigQuery concurrency limits is not fresh. Batch Priority freshness checks? <strong>Not fresh!</strong></p><p>If you're using Redshift, each freshness check will steal resources that could and should be used to load, model, and serve data. <strong>A freshness check that requires WLM tuning is definitely not fresh.</strong></p><p>If freshness checks are being executed with Airflow sensors, I'm going to conveniently avoid the subject. <strong>It's not fresh.</strong></p><p>And last but certainly not least: just because a table was loaded at T1 doesn't mean it was entirely up to date at that time. <strong>Fresh? Maybe. But also maybe not.</strong></p><h1>Measuring Freshness with Freshness Tables is Fresh.</h1><p>While certainly important and useful, record-level metadata columns are not always awesome. They certainly serve a purpose but freshness tables are better.</p><p>Freshness tables are not complicated and could look as simple as this:</p><div class="codeBlockContainer_MPoW theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_B9tL"><pre tabindex="0" class="prism-code language-text codeBlock__0OG thin-scrollbar"><code class="codeBlockLines_gEuF"><span class="token-line" style="color:#393A34"><span class="token plain">metadata=# \d freshness.tbl_freshness</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                               Table "freshness.tbl_freshness"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        Column        |           Type           | Collation | Nullable |                       Default</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">----------------------+--------------------------+-----------+----------+-----------------------------------------------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> id                   | integer                  |           | not null | nextval('freshness.tbl_freshness_id_seq'::regclass)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> fresh_at             | timestamp with time zone |           | not null |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> fqn                  | text                     |           | not null |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> table_catalog        | text                     |           | not null |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> table_schema         | text                     |           | not null |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> table_name           | text                     |           | not null |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> system               | text                     |           | not null |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> job_started_at       | timestamp with time zone |           | not null |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> job_ended_at         | timestamp with time zone |           | not null |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> job_duration_seconds | double precision         |           | not null |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> highwater_start      | timestamp with time zone |           | not null |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> highwater_end        | timestamp with time zone |           | not null |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Indexes:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    "tbl_freshness_catalog_idx" btree (table_catalog)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    "tbl_freshness_fresh_at_idx" btree (fresh_at)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    "tbl_freshness_schema_idx" btree (table_schema)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    "tbl_freshness_table_idx" btree (table_name)</span><br></span></code></pre><div class="buttonGroup_hRr1"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Source freshness is definitely cool. But what about freshness at all other stages of the data dependency graph? What about staging, base/intermediary/backroom model, fact, dim/scd, and presentation-layer freshness? What about data application freshness, or the master data tier? What about scheduled query freshness?</p><p>What about your metrics tier?</p><p>Measuring freshness across the entire graph is super important. Freshness tables let you do exactly that.</p><h1>How To Be Fresh</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="track-it">Track It.<a class="hash-link" href="#track-it" title="Direct link to heading">​</a></h3><p>Freshness is hard but tracking freshness doesn't have to be. And you'll have to start somewhere.</p><p>It is not a great feat of engineering to collect metadata within ingestion systems and persist that meta after each load iteration. Load instrumentation is fresh (and you should probably be doing it anyways).</p><p>It's pretty easy to add <a href="https://docs.getdbt.com/reference/resource-configs/pre-hook-post-hook" target="_blank" rel="noopener noreferrer">post-hooks</a> to DBT models. Calculating and persisting data model freshness is fresh.</p><p>Refreshing a materialized view? Add a "INSERT INTO metadata.tbl_freshness values (select something interesting)" line to the end of the statement, wrap it in a transaction, and voila. Instafresh. (And you might have <a href="https://docs.snowflake.com/en/sql-reference/functions/materialized_view_refresh_history.html" target="_blank" rel="noopener noreferrer">this already</a>.)</p><p>Use Airflow to manage data ingestion and transformation? Great! The power of <a href="https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/index.html#airflow.models.Base" target="_blank" rel="noopener noreferrer">on_success_callback</a> and <a href="https://composed.blog/airflow/execute-context" target="_blank" rel="noopener noreferrer">context</a> is already yours. Fresh.</p><p>At a loss for how to track freshness? Even a webhook-to-freshness-table can be fresh!</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="rock-it">Rock It.<a class="hash-link" href="#rock-it" title="Direct link to heading">​</a></h3><p>Believe it or not, many analysts and downstream engineers will self-service if they have low-friction ways for doing so.</p><p>Unfortunately they usually have to go to N different places, where N is the number of sources or systems in operation. Requiring stakeholders to know data systems inside-and-out, and requiring them to waste their time tracking down architectural decisions, is not fresh.</p><p>Freshness tables give people who are good at analytics the capability to do analytics on their insights. Meta? Yup. Powerful? Also yup. Efficient? Definitely.</p><p>My current employer has a single table of 31,630,551 freshness records, across thousands of distinct table FQN's and hundreds of sources. We know exactly when our data assets were historically fresh. We also know various platform upgrades successfully decreased time to insight, and which sources or data transformations were most impacted by those upgrades.</p><p>Ingestion and modeling systems currently persist freshness at a rate of ~10,000 records every hour, and we have several Tableau dashboards for tracking freshness deltas across categories.</p><p>Load and modeling optimizations are quickly surfaced, and the freshness feedback loop is on autopilot. It's fresh. And we rock it.</p><h1>Why Be Fresh?</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="freshness-is-the-goal">Freshness is the goal.<a class="hash-link" href="#freshness-is-the-goal" title="Direct link to heading">​</a></h3><p>Historical data is good, but fresh data is better. Especially when it is being used to make relevant, time-sensitive decisions. Known-and-communicated-fresh data? The best!</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="freshness-is-trust">Freshness is trust.<a class="hash-link" href="#freshness-is-trust" title="Direct link to heading">​</a></h3><p>When you're fresh and you can prove it, you retain significant trust. When you're fresh and others can prove it for you, it's even better. Freshness tables exposed to stakeholders allows them to tell you you're fresh (or otherwise).</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="freshness-is-seeing-systems-get-better-or-worse-and-learning-fast">Freshness is seeing systems get better or worse. And learning fast.<a class="hash-link" href="#freshness-is-seeing-systems-get-better-or-worse-and-learning-fast" title="Direct link to heading">​</a></h3><p>Data systems are complex, and the most important thing you can do when building complex systems is provide visibility into them. When anyone can debug history at any point in time, it's fresh. When you can prove, using historical freshness data, that engineering work is decreasing time to insight, it's fresh.</p><p>When you can see freshness getting worse as volume increases, new systems come online, or new platform requirements get added, it's fresh. When you can alert on freshness thresholds...... you get the idea.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="freshness-is-empowering">Freshness is empowering.<a class="hash-link" href="#freshness-is-empowering" title="Direct link to heading">​</a></h3><p>If it's not clear by now, knowing what systems are doing and what tables are fresh is incredibly empowering. It enables work prioritization, continuous system insights, proactive solutions, and innovation. Freshness facilitates data SLO's and SLA's, helps frame conversations around a common point of reference, and a whole lot more.</p><h1>Use Freshness To Keep Getting Fresh<!-- -->[r]<!-- -->.</h1><p>After seeing the past 6-7 months of freshness data I cannot stop thinking about the implications of it.</p><p>Communicating context in dashboards is fresh.</p><p>Getting away from time dependencies and proactively triggering model sub-graphs based on the knowledge that upstream dependencies are known-fresh is... fresh.</p><p>Tracking and reporting on time to insight is fresh.</p><p>Baselining various cost optimizations (Snowflake warehouse sizing?) is fresh.</p><p>And there are many more thoughts to come.</p><h1>So stay fresh out there.</h1>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[268 Billion Events With Snowplow and Snowflake at Cargurus]]></title>
        <id>268-billion-events-with-snowplow-snowflake-at-cargurus</id>
        <link href="https://your-docusaurus-test-site.com/268-billion-events-with-snowplow-snowflake-at-cargurus"/>
        <updated>2020-06-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Two years ago we set up Open-Source Snowplow at CarGurus to fulfill a need for self-managed client-side instrumentation. Since that time it has become incredibly impactful for the entire company and has scaled significantly beyond what was originally envisioned. The following is an overview of why we set up the system, our experience with it, what we have learned, and where we see it continuing to go.]]></summary>
        <content type="html"><![CDATA[<p>Two years ago we set up Open-Source Snowplow at CarGurus to fulfill a need for self-managed client-side instrumentation. Since that time it has become incredibly impactful for the entire company and has scaled significantly beyond what was originally envisioned. The following is an overview of why we set up the system, our experience with it, what we have learned, and where we see it continuing to go.</p><p>This post is quite long, so before getting too far into the details....</p><h1>The Stats At Time of Writing</h1><p>Total events collected: <strong>&gt;268 billion</strong></p><p>Data collected: &gt; <strong>1.5pb (uncompressed)</strong></p><p>Event volume: <strong>~ 1 billion events/day</strong></p><p>Max Daily Throughput: <strong>~15k events/second (sustained during peak hours)</strong></p><p>Distinct events: <strong>hundreds and hundreds</strong></p><p>Infrastructure upgrade duration: <strong>less than two minutes with zero downtime</strong></p><p>Distinct sites with Snowplow tracking: <strong>&gt;180</strong></p><p>So let's dive in.</p><h1>Why we Snowplow</h1><p>We chose Snowplow instead of building our own event tracking system, and plan to stick with it for the foreseeable future. Why?</p><p><strong>We wanted to independently manage and scale data collection systems.</strong></p><p>A conversation early in my tenure at CarGurus made a topic abundantly clear: many teams had a strong desire to separate concerns. At that time the company was growing quickly and demands were rapidly changing. Analysts and engineers were joining, onboarding fast, and asking completely new questions of our data. It was all-too-common to tell them their questions simply could not be answered due to the additional load tracking would place on production systems.
After being personally involved with a couple event-volume-related site outages, it was evident we needed to rethink data collection systems. We needed to be able to scale data collection infrastructure completely separately from application infrastructure, and we needed to significantly isolate the blast radius if something went wrong.</p><p>Snowplow was a strong ✅.</p><p><strong>We wanted to modernize event collection systems.</strong></p><p>Another requirement of choosing Snowplow was its ability to quickly and effectively introduce a lambda architecture into our analytics stack. As customer needs and demands evolved, we found that having both a real-time, low-latency (on the order of hundreds of milliseconds) component of the event pipeline as well as a batch-based, higher-latency (on the order of minutes) component gave us significant flexibility to proactively fulfill stakeholder asks before they surfaced.</p><p>Our pipelines are set up in AWS with Kinesis as transport and Snowflake as long-term data storage. As stakeholders need real-time access to enriched data, Kinesis is the go-to location. If stakeholders need access to historical event data or want to augment it with other sources in the data warehouse, Snowflake is the place to go.</p><p>Snowplow allowed us to introduce new ideas and methodologies and fulfill both point-in-time and future stakeholder needs. ✅</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="the-world-of-third-party-tracking-is-rapidly-changing-and-will-continue-to">The world of third-party tracking is rapidly changing and will continue to.<a class="hash-link" href="#the-world-of-third-party-tracking-is-rapidly-changing-and-will-continue-to" title="Direct link to heading">​</a></h3><p>As anyone involved in the world of client-side analytics is probably well-aware of, third-party tracking is increasingly going away. Initiatives like this and this and this are fantastic for online privacy (and commendable), but have major implications for tracking and analytics. With an eye towards the future we knew we needed to decrease reliance on Google Analytics, Adobe, Heap, and all other similar third-party tracking systems if we wanted full insight into web traffic. We wanted 100% of site behavior instead of sampled GA or ad-blocked Adobe. And we wanted to own the resulting data.</p><p>Snowplow enabled us to track first-party web activity and regain full visibility into usage behavior on our site. ✅</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="snowplow-sets-a-high-bar-for-fault-tolerance-redundancy-and-durability">Snowplow sets a high bar for fault-tolerance, redundancy, and durability.<a class="hash-link" href="#snowplow-sets-a-high-bar-for-fault-tolerance-redundancy-and-durability" title="Direct link to heading">​</a></h3><p>When set up properly, the Snowplow infrastructure is extremely fault-tolerant. We set up ours in AWS and have not regretted it whatsoever.
Javascript tracking code buffers unsent events locally in the case of collection infrastructure being down. AWS application load balancers are multi-AZ and AWS maintains a good SLA for them. All access logs are retained for a period of time in case the load balancer is up but the rest of the system is down. Kinesis replicates data across three AZ's by default, and retention can be configured up to 168 hours in the case of mid-pipeline enrichment outages. S3 is highly durable and highly available by default. And the list goes on....</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="we-wanted-to-own-all-data-that-was-generated">We wanted to own all data that was generated.<a class="hash-link" href="#we-wanted-to-own-all-data-that-was-generated" title="Direct link to heading">​</a></h3><p>As mentioned above, if we were going to invest time and money building scalable event-collection infrastructure, we simply had to own the outcome. It is a significant investment to get large-scale infrastructure and instrumentation off the ground, and it's hard to do tracking in a way that doesn't impose a burden on analysts down the road (think Google Analytics events).
We didn't want to deal with API rate limiting when attempting to recover our data. We didn't want to be tied to BigQuery, and we didn't want our customers' data flowing through other companies' systems.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="we-needed-to-keep-data-quality-high">We needed to keep data quality high.<a class="hash-link" href="#we-needed-to-keep-data-quality-high" title="Direct link to heading">​</a></h3><p>When building out data collection and processing systems, it's one thing to set up infrastructure. It's a completely different story to keep quality high as priorities, teams, and customer demands shift. It's even harder to impose more work in an attempt to keep data "clean".</p><p>The way snowplow handles event validation and stream-redirection was a perfect fit in our case. After setting the system up we quickly found that leveraging Self-Describing Events and the associated jsonschemas empowered us to enforce data quality in a very low-friction way. Whenever events start flowing (with a bad push, etc) that don't adhere to the implementing party's jsonschema, they are redirected out of the "good" path for immediate review and recovery. Self-describing events (and jsonschemas) also gave us the ability to do some pretty neat automation, which will be covered later.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="the-world-of-compliance-is-changing-rapidly-and-we-needed-the-ability-to-audit-every-piece-of-information">The world of compliance is changing rapidly and we needed the ability to audit every piece of information.<a class="hash-link" href="#the-world-of-compliance-is-changing-rapidly-and-we-needed-the-ability-to-audit-every-piece-of-information" title="Direct link to heading">​</a></h3><p>Since we first set up Snowplow the world has been forced to navigate two big data-privacy hurdles: GDPR and CCPA. Since we own and operate all collection infrastructure we did not need to get a third-party vendor DPA. Snowplow gives a significant amount of flexibility for enriching and redirecting PII, as well as configurable pseudonymization. It's good, it "just works", and it has saved us a considerable amount of time.</p><p>We've also spent a significant amount of time persisting full system lineage when splitting events into Snowflake which makes it easy to fully adhere to privacy legislation and associated requirements.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="we-wanted-to-build-guide-and-get-out-of-the-way">We wanted to build, guide, and get out of the way.<a class="hash-link" href="#we-wanted-to-build-guide-and-get-out-of-the-way" title="Direct link to heading">​</a></h3><p>My favorite part about programming and tech is building (machine) processes, cultivating good (human) habits, and getting out of the way. An early challenge was maintaining clear, consistent definitions of data between frontend engineers (the implementer of tracking) and analysts (the stakeholders of said tracking). Event data was previously tracked, flowed through a number of different systems, and was often renamed (sometimes several times) when it finally landed in Snowflake.. Snowplow's self-describing events have allowed us to shift many conversations away from data pipeline engineers and back to where they should be taking place. I was able to back away since stakeholders and implementers were able to approach important conversations from a point of shared, consistent understanding.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="last-but-not-least-we-needed-to-hit-the-ground-running">Last but not least, we needed to hit the ground running.<a class="hash-link" href="#last-but-not-least-we-needed-to-hit-the-ground-running" title="Direct link to heading">​</a></h3><p>While we may have had time to build out our own system from scratch, why build when you can lean on the backs of giants? The community of people contributing to Snowplow has grown significantly since the first time I implemented it, and people all over the world have run into almost every challenge imaginable. As fun as it would have been to deconstruct the system and build and implement our own, it simply wasn't worth it. Our stakeholders had questions that they wanted answered "yesterday" and their needs were our priority #1.</p><p>Snowplow enabled us to hit the ground running and answer real business questions "now". ✅</p><h1>Our Journey</h1><h2 class="anchor anchorWithStickyNavbar_mojV" id="phase-1-keep-it-simple-get-it-running-prove-the-systems-worth">Phase 1: Keep It Simple. Get It Running. Prove the System's Worth.<a class="hash-link" href="#phase-1-keep-it-simple-get-it-running-prove-the-systems-worth" title="Direct link to heading">​</a></h2><p>When Snowplow was first rolled out there was a single mission in mind: make sure it was a good organizational fit. We had systems in place with some functional overlap already, so we had to make sure we weren't introducing a new system simply........ for the sake of introducing a new system. Or staking claim in others' territory. The system had to work. It had to answer questions that had been previously-unanswerable. It had to be robust. And it had to solve real-life challenges.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="we-very-consciously-chose-to">We very consciously chose to:<a class="hash-link" href="#we-very-consciously-chose-to" title="Direct link to heading">​</a></h3><p><strong>Automate all infrastructure.</strong> We wanted to scale, upgrade, and roll out entirely new sets of infrastructure fast. Everything was Terraformed and infrastructure was 100% immutable.</p><p><strong>Keep it simple.</strong> We could have gone straight to k8s or autoscaling clusters or ecs or eks or any other new (admittedly attractive) thing. But we didn't. We wanted to see how the system itself would perform and scale.</p><p><strong>Focus on the stakeholder.</strong> The only way this initiative was going to work was if others felt ownership of it, it decreased their pain, it empowered them to do their jobs better, or a combination of all. Instead of focusing on a hip/cool/hot/new system we focused on answering the questions at hand.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="phase-2-ramp-adoption-answer-more-questions">Phase 2: Ramp Adoption. Answer More Questions.<a class="hash-link" href="#phase-2-ramp-adoption-answer-more-questions" title="Direct link to heading">​</a></h2><p>Once the system was set up and a minimal set of events were implemented across our site, we started really putting the system to work. We went around the company figuring out what questions people had been historically struggling to answer, and then helped implement the necessary tracking to answer these questions. We also pushed the system in other ways, such as seeing how effective it was for third-party tracking on other websites, redirect tracking, no-js tracking, and more.</p><p>We originally rolled out most of the tracking via structured events. While great for adoption, it was evident things would quickly get out of hand before realizing the full value of the system if we continued down this path. Json-encoded strings with arbitrary values were being passed as se_property. Event definitions were unclear and quickly inconsistent. Snowflake query duration (and therefore cost) was going through the roof as people tried to pull data out of a single poorly-clustered "struct_events" table. And more.</p><p>While we were able to answer questions, it was clear this methodology of doing so would be unsustainable and/or expensive long term.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="phase-3-go-all-in-on-mandating-data-quality">Phase 3: Go All-In on Mandating Data Quality.<a class="hash-link" href="#phase-3-go-all-in-on-mandating-data-quality" title="Direct link to heading">​</a></h2><p>After quickly learning the pain points associated with struct-event tracking in a company our size we had a decision to make: <strong>go all-in on self-describing events or bust.</strong></p><p>Admittedly, mandating that events only get successfully delivered when accompanied by a jsonschema is a bit of overhead that is passed to the implementer. In this case, quality comes at a small time cost. But said cost is more than worth it.</p><p><strong>Why it's worth using schemas:</strong></p><ul><li>Data can be explicitly defined and versioned. Schema evolution is possible without blowing up the world of analytics.</li><li>Self-describing events can be independently monitored and alerted on.</li><li>Incoming events are validated and redirected in-transit.</li><li>Jsonschemas establish consistency and empower 1:1 communication.</li><li>Significant downstream automation can be built on top of schemas. Auto-migrating tables, splitting/deduplicating events, and intelligently clustering tables in snowflake are only possible with self-describing events.</li><li>"Bad" event volume can be alerted on and directed to the respective team.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="phase-4-solidify-infrastructure-add-necessary-complexity-automate-data-engineering-toil-away">Phase 4: Solidify Infrastructure. Add Necessary Complexity. Automate Data Engineering Toil Away.<a class="hash-link" href="#phase-4-solidify-infrastructure-add-necessary-complexity-automate-data-engineering-toil-away" title="Direct link to heading">​</a></h2><p>Once we knew the system would work and that it was a good organizational fit, the next step was to make the supporting infrastructure rock-solid.</p><p>Originally setting up the system in the simplest manner possible meant that we willingly accepted tradeoffs and some risk. We had to make sure we had enough machines behind the load balancer to service traffic at any point in time. We had to closely (and sometimes manually!) monitor the enrichment and s3 loader machines. And we had to manually scale supporting infrastructure when necessary. From a cost and utilization perspective, we ran the system slightly over-provisioned during peak volume and significantly over-provisioned during the slowest points of the day. The simplicity came at a cost tradeoff, but again - there's very little point in optimizing a system that you're not going to continue using.</p><p>As we became more confident in the long-term direction of this project it was time to buckle down and set up all infrastructure in a way that we could step away from. This ultimately meant containerizing all systems, introducing auto-scaling groups with reasonable scaling policies, auto-scaling dynamodb, figuring out how to reasonably auto-scale kinesis, and a number of other not-too-complicated-but-incredibly-important devops things. We decided to run EC2 ASG's of flatcar container linux vs running the containers on ECS, EKS, or a k8s cluster, though one of those will probably be in our future.</p><p>Another critical aspect of this automation was the implementation of a load/deduplicate/split/auto-cluster system. We have pipelines geolocated all over the world and load all data into Snowflake. By leveraging self-describing event json-schemas mandated in Phase 2, we were able to auto-migrate Snowflake tables, auto-split events into said tables, efficiently deduplicate events, strategically cluster, and much much more.</p><p>The automation here has saved us considerable amounts of time and money, and has allowed us to go incredibly far by automating ourselves away.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="phase-5-decrease-implementation-overhead-dont-sacrifice-data-quality">Phase 5: Decrease Implementation Overhead. Don't Sacrifice Data Quality.<a class="hash-link" href="#phase-5-decrease-implementation-overhead-dont-sacrifice-data-quality" title="Direct link to heading">​</a></h2><p>This is the phase we are in currently and it's a very important one. As a data engineer, if you get data quality wrong (even unintentionally!), it can easily lead a company in a bad direction and will create massive problems that take years to resolve. We did not want to make these mistakes, so we quickly pivoted towards strictly prioritizing data quality. But we believe that's not the end of the story. If we can mandate data quality while making schemas as easy to implement, deploy, evolve, and monitor as not having them at all, we'll consider it a massive success. We also strive towards establishing clear, consistent, sharable definitions for all and make all definitions easily searchable.</p><p>At this point we are well on our way to achieving these goals. But we aren't quite there yet.</p><h1>What we have done a bit differently</h1><p>The snowplow stream collector, the stream enricher, and the s3 loader are basically out-of-the-box and have been configured to suit our needs and volume. But we've gone a slightly differently direction with other core aspects of the system.</p><p>We decided not to use the snowplow snowflake loader largely due to avoiding additional dependencies, needing more flexibility, and desiring to process and store data consistently to other internal systems. We have a significant amount of professional experience with Snowflake administration/automation, and saw opportunity to make our lives a bit easier long term if we rolled this portion of the pipeline ourselves.</p><p>We decided not to use the iglu schema repository (but do use jsonschemas for self-described events!) due to identifying numerous opportunities at this level. This functionality may be covered in another future blog post.</p><p>We love Graylog, fluent-bit, and a handful of other tools. So we've incorporated them in useful ways.</p><h1>What we have learned along the way</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="stakeholder-empowerment-is-1">Stakeholder empowerment is #1.<a class="hash-link" href="#stakeholder-empowerment-is-1" title="Direct link to heading">​</a></h3><p>I cannot stress this enough: the only way to make initiatives like this work is by taking a customer-focused, stakeholder-oriented approach. As a data engineer my customers are typically analysts or various business intelligence initiatives. But as data maturity grows in the organization the word "stakeholder" evolves to also include other internal systems, and then SEO optimization, potentially strategic partnerships with other orgs, external customers, and much more.</p><p>By running Snowplow and leaning into significant Snowflake automation my role has become less gatekeeper and more facilitator, working alongside stakeholders to achieve the respective end goal.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="infrastructure-and-system-automation-is-2">Infrastructure and system automation is #2.<a class="hash-link" href="#infrastructure-and-system-automation-is-2" title="Direct link to heading">​</a></h3><p>Setting up and thoroughly understanding the Snowplow stack on AWS is non-trivial. There's a significant number of moving pieces and a deep understanding of Kinesis internals is required to manage it well. We automated everything from the very start so spinning up and properly configuring 70+ AWS resources per pipeline becomes (almost) trivial.</p><p>Another benefit of full infrastructure automation is the ability to shut systems down cleanly when we don't need them anymore, and make code central to infrastructure rollouts.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="explicitly-mandating-data-quality-has-numerous-benefits-it-may-come-at-a-cost-but-it-doesnt-have-to">Explicitly mandating data quality has numerous benefits. It may come at a cost, but it doesn't have to.<a class="hash-link" href="#explicitly-mandating-data-quality-has-numerous-benefits-it-may-come-at-a-cost-but-it-doesnt-have-to" title="Direct link to heading">​</a></h3><p>Leaning into 100% self-describing Snowplow events admittedly comes with operational overhead. "Bad" must scale as cleanly as "good" in the case that a high number of events are redirected. Otherwise, the "pipes" will become constricted and the entire system will back up. Event schemas must be properly-validated json before being deployed into production pipelines. Front-end engineers must be personally invested in helping to mandate data quality, and using jsonschemas to validate each piece of instrumentation is new cognitive load.</p><p>We've leaned into automation and/or internal tooling here as well to help reduce this overhead, but there's much further to go.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="the-snowplowsnowflake-combination-scales-extremely-well-it-has-empowered-us-to-answer-many-previously-unanswerable-questions-and-has-opened-up-numerous-doors-of-opportunity">The Snowplow+Snowflake combination scales extremely well. It has empowered us to answer many previously-unanswerable questions and has opened up numerous doors of opportunity.<a class="hash-link" href="#the-snowplowsnowflake-combination-scales-extremely-well-it-has-empowered-us-to-answer-many-previously-unanswerable-questions-and-has-opened-up-numerous-doors-of-opportunity" title="Direct link to heading">​</a></h3><p>I have personally run open-source Snowplow analytics a large number of times with AWS Redshift as a data warehouse. While functional, it quickly becomes a burden to maintain and scale. The Snowplow+Snowflake combination is extremely effectively and extremely powerful. And scales well, even at CarGurus' volume.</p><h1>Where we see it continuing to go in the future</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="leverage-the-stream-luke">Leverage the stream, Luke<a class="hash-link" href="#leverage-the-stream-luke" title="Direct link to heading">​</a></h3><p>There's so much more opportunity to leverage data in-transit that we've barely cracked the surface of. Streaming databases such as Materialize and KSQL show massive promise for continuing to decrease time-to-insight and are some of the most exciting data projects since PipelineDB. I'm thrilled at the ability to bolt functionality onto an existing system (vs. re-architecting it from scratch as demands change) and am very interested to see what the low-latency future holds.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="continued-focus-on-auditability-provenance-and-data-governance">Continued focus on auditability, provenance, and data governance.<a class="hash-link" href="#continued-focus-on-auditability-provenance-and-data-governance" title="Direct link to heading">​</a></h3><p>Have I mentioned self-describing events with jsonschemas are amazing? Another significant benefit of them is the ability to cleanly, quickly, and effectively track provenance from upstream systems, all the way through the pipeline, to a data warehouse. Provenance is quite easy.</p><p>When it comes to data governance and auditing, we took numerous intentional steps in our Snowflake loader to make finding and purging data upstream doable. Snowflake's  METADATA$FILENAME  and  METADATA$FILE_ROW_NUMBER  functionality means that if anyone requests removal of data (per GDPR) or requests data not be sold (per CCPA), it can be purged or excluded from both Snowflake as well as upstream flat files sitting in S3/Glacier/etc.</p><p>Continue to empower stakeholders, developers, analysts</p><p>Last but certainly not least, identifying and eliminating points of misunderstanding, cognitive load, or implementation/development challenges remains top priority. "Stakeholders" may be fellow engineers, they may be analysts whose job is to use the data we're collecting and warehousing, or they may be product owners whose job and decision-making process relies on our data.</p><p>We'll continue to push forward to make schema creation, discovery, evolution, and management more intuitive and fun.</p><p>We'll continue to make real-time analytics a reality.</p><p>We'll continue to make event-level observability and anomaly detection a critical part of instrumentation.</p><p>We'll continue to push forward with automating ourselves out of the way.</p><p>And we'll keep trying our best to make others' lives easier.</p><h1>In Conclusion</h1><p>If there's any confusion by this point, the Snowplow and Snowflake combination has worked incredibly well for CarGurus. The company has leveraged these systems to blow open doors of opportunity and the system has proven itself time and time again over the past couple years.</p><p>Looking backwards, I'm quite happy with the rollout and have certainly learned a lot.</p><p>Looking forward I see only pure potential.</p>]]></content>
        <author>
            <name>Jake</name>
        </author>
        <category label="snowplow" term="snowplow"/>
        <category label="snowflake" term="snowflake"/>
        <category label="cargurus" term="cargurus"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[How To Install and Configure SnowSQL]]></title>
        <id>how-to-install-and-configure-snowsql</id>
        <link href="https://your-docusaurus-test-site.com/how-to-install-and-configure-snowsql"/>
        <updated>2019-12-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[SnowSQL is the command-line interface for accessing your Snowflake instance.]]></summary>
        <content type="html"><![CDATA[<p><a href="https://docs.snowflake.com/en/user-guide/snowsql.html" target="_blank" rel="noopener noreferrer">SnowSQL</a> is the command-line interface for accessing your Snowflake instance.</p><p>The following is a quick "how to" guide for setting it up.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="installation">Installation<a class="hash-link" href="#installation" title="Direct link to heading">​</a></h3><p>After logging into your Snowflake web interface, the SnowSQL installer is available via <code>Help</code> -&gt; <code>Download</code>:</p><p><img loading="lazy" alt="download" src="/assets/images/01_help_download-51bd2775f280fdd3c8e8de03e82b59fc.png" width="720" height="426" class="img_E7b_"></p><p>You'll need to select the appropriate version for your machine:</p><p><img loading="lazy" alt="cli" src="/assets/images/02_snowcli-cb05330d22257337709910d385964da5.png" width="720" height="409" class="img_E7b_"></p><p>..and install it:</p><p><img loading="lazy" alt="install" src="/assets/images/03a_install-82937641bba4ac10cc6733e3a545dcce.png" width="720" height="538" class="img_E7b_"></p><p><img loading="lazy" alt="install-success" src="/assets/images/03b_install-2402bae97e26875cf0192c4b0a82925a.png" width="720" height="518" class="img_E7b_"></p><p>To verify installation, simply open a terminal window and run snowsql. If installed properly, you will receive a list of connection and option flags:</p><div class="codeBlockContainer_MPoW theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_B9tL"><pre tabindex="0" class="prism-code language-text codeBlock__0OG thin-scrollbar"><code class="codeBlockLines_gEuF"><span class="token-line" style="color:#393A34"><span class="token plain">$ snowsql</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Usage: snowsql [OPTIONS]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Options:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -a, --accountname TEXT          Name assigned to your Snowflake account. If</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      you are not on us-west-2 or AWS deployement,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      append the region and platform to the end,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      e.g., &lt;account&gt;.&lt;region&gt; or</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      &lt;account&gt;.&lt;region&gt;.&lt;platform&gt;Honors</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      $SNOWSQL_ACCOUNT.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -u, --username TEXT             Username to connect to Snowflake. Honors</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      $SNOWSQL_USER.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -d, --dbname TEXT               Database to use. Honors $SNOWSQL_DATABASE.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -s, --schemaname TEXT           Schema in the database to use. Honors</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      $SNOWSQL_SCHEMA.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -r, --rolename TEXT             Role name to use. Honors $SNOWSQL_ROLE.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -w, --warehouse TEXT            Warehouse to use. Honors $SNOWSQL_WAREHOUSE.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -h, --host TEXT                 Host address for the connection. Honors</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      $SNOWSQL_HOST.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -p, --port INTEGER              Port number for the connection. Honors</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      $SNOWSQL_PORT.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      --region TEXT                   (DEPRECATED) Append the region or any sub</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      domains before snowflakecomputing.com to the</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      end of accountname parameter after a dot.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      e.g., accountname=&lt;account&gt;.&lt;region&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -m, --mfa-passcode TEXT         Token to use for multi-factor authentication</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      (MFA)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      --mfa-passcode-in-password      Appends the MFA passcode to the end of the</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      password.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      --abort-detached-query          Aborts a query if the connection between the</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      client and server is lost. By default, it</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      won't abort even if the connection is lost.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      --probe-connection              Test connectivity to Snowflake. This option</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      is mainly used to print out the TLS/SSL</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      certificate chain.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      --proxy-host TEXT               (DEPRECATED. Use HTTPS_PROXY and HTTP_PROXY</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      environment variables.) Proxy server</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      hostname. Honors $SNOWSQL_PROXY_HOST.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      --proxy-port INTEGER            (DEPRECATED. Use HTTPS_PROXY and HTTP_PROXY</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      environment variables.) Proxy server port</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      number. Honors $SNOWSQL_PROXY_PORT.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      --proxy-user TEXT               (DEPRECATED. Use HTTPS_PROXY and HTTP_PROXY</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      environment variables.) Proxy server</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      username. Honors $SNOWSQL_PROXY_USER. Set</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      $SNOWSQL_PROXY_PWD for the proxy server</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      password.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      --authenticator TEXT            Authenticator: 'snowflake',</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      'externalbrowser' (to use any IdP and a web</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      browser), or</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      https://&lt;your_okta_account_name&gt;.okta.com</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      (to use Okta natively).</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -v, --version                   Shows the current SnowSQL version, or uses a</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      specific version if provided as a value.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      --noup                          Disables auto-upgrade for this run. If no</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      version is specified for -v, the latest</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      version in ~/.snowsql/ is used.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -D, --variable TEXT             Sets a variable to be referred by &amp;&lt;var&gt;. -D</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      tablename=CENUSTRACKONE or --variable</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      db_key=$DB_KEY</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -o, --option TEXT               Set SnowSQL options. See the options</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      reference in the Snowflake documentation.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -f, --filename PATH             File to execute.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -q, --query TEXT                Query to execute.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      --config PATH                   Path and name of the SnowSQL configuration</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      file. By default, ~/.snowsql/config.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -P, --prompt                    Forces a password prompt. By default,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      $SNOWSQL_PWD is used to set the password.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -M, --mfa-prompt                Forces a prompt for the second token for</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      MFA.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -c, --connection TEXT           Named set of connection parameters to use.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      --single-transaction            Connects with autocommit disabled. Wraps</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      BEGIN/COMMIT around statements to execute</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      them as a single transaction, ensuring all</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      commands complete successfully or no change</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      is applied.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      --private-key-path PATH         Path to private key file in PEM format used</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      for key pair authentication. Private key</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      file is required to be encrypted and</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      passphrase is required to be specified in</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      environment variable</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      $SNOWSQL_PRIVATE_KEY_PASSPHRASE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -U, --upgrade                   Force upgrade of SnowSQL to the latest</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      version.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -K, --client-session-keep-alive</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      Keep the session active indefinitely, even</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      if there is no activity from the user..</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      --disable-request-pooling       Disable request pooling. This can help speed</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                      up connection failover</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      -?, --help                      Show this message and exit.</span><br></span></code></pre><div class="buttonGroup_hRr1"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h1>Configuration</h1><p>As indicated above, SnowSQL has a host of connection params and settings, and allows variable declaration and substitution. You won't need to be familiar with all the options to hit the ground running, but I definitely recommend leveraging ~/.snowsql/config to persist your connection details and personal preferences.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="add-your-connection-details-to-the-connections-section-of-snowsqlconfig">Add your connection details to the <code>connections</code> section of <code>~/.snowsql/config</code>.<a class="hash-link" href="#add-your-connection-details-to-the-connections-section-of-snowsqlconfig" title="Direct link to heading">​</a></h3><p>The first few lines of your ~/.snowsql/config file should look like the following:</p><div class="codeBlockContainer_MPoW theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_B9tL"><pre tabindex="0" class="prism-code language-text codeBlock__0OG thin-scrollbar"><code class="codeBlockLines_gEuF"><span class="token-line" style="color:#393A34"><span class="token plain">[connections]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">accountname = YOUR_ACCOUNT_NAME</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">username = YOUR_USERNAME</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">password = YOUR_PASSWORD</span><br></span></code></pre><div class="buttonGroup_hRr1"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Please note! There are some caveats regarding quote-wrapping special characters and escaping quotes within passwords. For more information, please consult the <a href="https://docs.snowflake.com/en/user-guide/snowsql-config.html" target="_blank" rel="noopener noreferrer">docs located here</a>.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="add-your-personal-preferences-to-the-options-section">Add your personal preferences to the <!-- -->[options]<!-- --> section.<a class="hash-link" href="#add-your-personal-preferences-to-the-options-section" title="Direct link to heading">​</a></h3><p>I'm OK with most of the configuration defaults, so the only option I typically modify is sfqid. It enables output of snowflake query id's in the summary, which can be quite helpful:</p><div class="codeBlockContainer_MPoW theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_B9tL"><pre tabindex="0" class="prism-code language-text codeBlock__0OG thin-scrollbar"><code class="codeBlockLines_gEuF"><span class="token-line" style="color:#393A34"><span class="token plain">    [options]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    sfqid = True</span><br></span></code></pre><div class="buttonGroup_hRr1"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong>Want to be unfriendly? Add:</strong></p><div class="codeBlockContainer_MPoW theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_B9tL"><pre tabindex="0" class="prism-code language-text codeBlock__0OG thin-scrollbar"><code class="codeBlockLines_gEuF"><span class="token-line" style="color:#393A34"><span class="token plain">friendly = False</span><br></span></code></pre><div class="buttonGroup_hRr1"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong>Want to write to a specific log location? Add:</strong></p><div class="codeBlockContainer_MPoW theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_B9tL"><pre tabindex="0" class="prism-code language-text codeBlock__0OG thin-scrollbar"><code class="codeBlockLines_gEuF"><span class="token-line" style="color:#393A34"><span class="token plain">log_file = your/path/to/log</span><br></span></code></pre><div class="buttonGroup_hRr1"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong>Want to be difficult? Add:</strong></p><div class="codeBlockContainer_MPoW theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_B9tL"><pre tabindex="0" class="prism-code language-text codeBlock__0OG thin-scrollbar"><code class="codeBlockLines_gEuF"><span class="token-line" style="color:#393A34"><span class="token plain">editor = emacs</span><br></span></code></pre><div class="buttonGroup_hRr1"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>(just kidding)</p><p>You get the idea. Your mileage may vary.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="lock-the-file-down-to-you-and-only-you">Lock the file down to you and only you.<a class="hash-link" href="#lock-the-file-down-to-you-and-only-you" title="Direct link to heading">​</a></h3><p>If you've been paying attention, you have already realized sensitive credentials are stored in plaintext on your machine. This is not a reason to ?, and is similar to the postgres pgpass file. You'll want to lock it down.</p><p><code>chmod 400 ~/.snowsql/config</code></p><h1>Summary</h1><p>SnowSQL is pretty easy to set up and start using, and you'll probably find it quickly becomes critical for development and database administration workflows. After installing the tool and becoming familiar with how it is configured and utilized, you'll be well on your way to a pleasant experience.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[GDPR for Engineers - What You Need to Know]]></title>
        <id>gdpr-for-engineers-what-you-need-to-know</id>
        <link href="https://your-docusaurus-test-site.com/gdpr-for-engineers-what-you-need-to-know"/>
        <updated>2019-05-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[GDPR was approved by EU parliament on April 14, 2016, went into effect May 25, 2018, and impacts any business handling any personal data of any EU resident.]]></summary>
        <content type="html"><![CDATA[<p>GDPR was approved by EU parliament on April 14, 2016, went into effect May 25, 2018, and impacts any business handling any personal data of any EU resident.</p><p>At a high level, GDPR is a directive on the protection of personal data and can be scoped twofold. First, the law protects persons concerned by processing of personal data. Second, it enforces additional accountability on businesses involved in the processing of personal data.</p><p>What does GDPR do exactly? Let's dive in.</p><h1>GDPR increases territorial scope</h1><p>The law applies to all companies processing personal data of subjects residing in the EU. Again, if your systems handle any PII of any person residing in the European Union, you are responsible to comply with any and all privacy regulation.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="are-you-a-boston-based-business-and-serve-eu-customers-when-they-visit-as-tourists-every-summer">Are you a Boston-based business and serve EU customers when they visit as tourists every summer?<a class="hash-link" href="#are-you-a-boston-based-business-and-serve-eu-customers-when-they-visit-as-tourists-every-summer" title="Direct link to heading">​</a></h3><p>You need to comply.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="are-you-a-us-based-business-with-a-european-presence">Are you a US-based business with a European presence?<a class="hash-link" href="#are-you-a-us-based-business-with-a-european-presence" title="Direct link to heading">​</a></h3><p>You need to comply.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="are-you-a-european-based-business-with-no-international-presence">Are you a European-based business with no international presence?<a class="hash-link" href="#are-you-a-european-based-business-with-no-international-presence" title="Direct link to heading">​</a></h3><p>You need to comply.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="are-you-a-european-based-business-with-significant-international-presence">Are you a European-based business with significant international presence?<a class="hash-link" href="#are-you-a-european-based-business-with-significant-international-presence" title="Direct link to heading">​</a></h3><p>You need to comply.</p><h1>GDPR mandates explicit consent when tracking behavior</h1><p>Implicit consent, which is common in the United States, is not adequate according to GDPR.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="consent-language-must-be-clear-accessible-and-intelligible">Consent language must be clear, accessible, and intelligible.<a class="hash-link" href="#consent-language-must-be-clear-accessible-and-intelligible" title="Direct link to heading">​</a></h3><p>Instead of privacy policies being filled with often-intelligible legalese, GDPR mandates that privacy policies, cookie banners, or other forms of gathering tracking consent must use clear and accessible language which makes the purpose and scope easily understood.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="purpose-of-consent-must-be-attached">Purpose of consent must be attached.<a class="hash-link" href="#purpose-of-consent-must-be-attached" title="Direct link to heading">​</a></h3><p>Vaguely requesting consent for "cookies" is no longer enough - GDPR states the purpose of the tracking be expressed. If your systems (like remarketing, advertising, or event tracking pixels) persist site visitor PII, you now have to provide the visitors with a reason for said tracking.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="tracking-must-be-accepted-and-is-opt-out-by-default">Tracking must be accepted (and is opt-out by default).<a class="hash-link" href="#tracking-must-be-accepted-and-is-opt-out-by-default" title="Direct link to heading">​</a></h3><p>In the United States, tracking is typically opt-in by default with a notice effectively stating, "if you continue to use this site, you have implicitly allowed us to track you". This is not acceptable according to GDPR, as tracking must be opt-out by default and the site visitor must explicitly give their consent to be tracked.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="it-must-be-as-easy-to-withdraw-consent-as-it-is-to-give-it">It must be as easy to withdraw consent as it is to give it.<a class="hash-link" href="#it-must-be-as-easy-to-withdraw-consent-as-it-is-to-give-it" title="Direct link to heading">​</a></h3><p>Since post-GDPR tracking is opt-out by default and a user must give consent with full knowledge of the intended purpose, companies are naturally incentivized to make it as easy as possible to give consent. But wait, there's more! Once consent has been granted, it must be as easy for a user to revoke consent as it was to initially give it.</p><h1>GDPR provides specific rights for data subjects.</h1><p>Not entirely unlike the United States' Bill of Rights or the United Nations' Universal Declaration of Human Rights, GDPR establishes a notion of "basic digital rights" for data subjects. These rights are outlined as follows:</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="breach-notification">Breach notification<a class="hash-link" href="#breach-notification" title="Direct link to heading">​</a></h3><p>According to GDPR, all individuals have a right to be notified if a company experiences a breach involving their personal data. This breach notification process is mandatory if the breach is likely to result in any risk to an individual's rights or freedoms. It must also be done within 72 hours of first awareness of said breach. If your company is a data processor that serves controllers, you must notify those controllers in the same manner.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="right-to-access">Right to access<a class="hash-link" href="#right-to-access" title="Direct link to heading">​</a></h3><p>Data subjects can now obtain confirmation from a controller as to whether their personal data is being processed, where it is being processed, and what purpose the processing serves. They can also request the controller provide a copy of all personal data, free of charge, in a "common electronic format".</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="right-to-be-forgotten">Right to be forgotten<a class="hash-link" href="#right-to-be-forgotten" title="Direct link to heading">​</a></h3><p>This "basic digital right" really means one thing: upon request of an individual, a company must erase all personal data generated by or relating to that individual. The controller must cease dissemination of said data, and third parties must halt the processing of it.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="data-portability">Data portability<a class="hash-link" href="#data-portability" title="Direct link to heading">​</a></h3><p>Lastly, a data subject can request all personal data concerning them and retains the full right to directly transfer the data to another controller. The data must be generated and transferred in a common format.</p><h1>GDPR mandates privacy by design.</h1><p>A privacy-first approach to data and systems engineering is another major component of GDPR. The law is written rather vaguely (since it's very difficult to give overarching directives on how a system should be built) and states, "a controller should implement appropriate technical and organizational measures" for securing personal data. It also declares a controller should "hold and process only necessary data", and should "limit access to personal data to those doing the processing" of it.</p><p>Ensuring your systems have been built in a way that enables your business to comply with the above is no small feat.</p><h1>GDPR establishes data protection officers.</h1><p>If your core business activities consist of processing operations which require regular, systematic monitoring of subjects on a larger scale, special categories of data, or data pertaining to criminal offenses, a data protection officer must be designated.</p><p>This officer should be appointed on the basis of professional qualities and expert knowledge of data protection law and practices, must be a staff member or external service provider, and must report directly to the highest level of management.</p><p>The data protection officer must be provided appropriate resources to carry out tasks and maintain expert knowledge and must not carry out tasks that could result in a potential conflict of interest.</p><p>Finally, contact information of the data protection officer must be included in all relevant DPA's.</p><h1>GDPR defines the enforcement of non-compliance.</h1><p>What happens when you don't comply with GDPR? The law makes it pretty clear: organizations can be fined up to €20 million or 4% of annual worldwide revenue for the prior year (whichever is greater). GDPR fines are tiered and apply to both controllers and processors.</p><p>Clouds are not exempt from enforcement.</p><h1>In conclusion</h1><p>When it comes to processing and storing data, GDPR changes a lot of things. Properly complying with the regulation listed above is non-trivial, but ultimately gives individuals basic digital rights. As an engineer, it's incredibly important to understand the provisions listed and their implications on the systems you build.</p><p>Would you be able to secure a breach, identify all affected parties, and communicate the magnitude within 72 hours?</p><p>Do you have a good audit of all personal data moving through your systems (or third-party systems)?</p><p>Would you be able to provide a data subject with a copy of all of their data if asked to do so?</p><p>Would you be able to truly erase all personal data of an individual, if requested?</p><p>It's hard, but is by no means impossible.</p><h1>Further reading</h1><p>I've found the following resources to be useful when learning about GDPR law (and the implications of it when building systems):</p><ul><li><a href="https://eugdpr.org/" target="_blank" rel="noopener noreferrer">https://eugdpr.org/</a></li><li><a href="https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/data-protection/2018-reform-eu-data-protection-rules_en" target="_blank" rel="noopener noreferrer">https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/data-protection/2018-reform-eu-data-protection-rules_en</a></li><li><a href="https://ec.europa.eu/info/law/law-topic/data-protection_en" target="_blank" rel="noopener noreferrer">https://ec.europa.eu/info/law/law-topic/data-protection_en</a></li></ul>]]></content>
        <author>
            <name>Jake</name>
        </author>
        <category label="gdpr" term="gdpr"/>
        <category label="engineers" term="engineers"/>
        <category label="data privacy" term="data privacy"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[GDPR for Engineers - What is Personal Data?]]></title>
        <id>gdpr-for-engineers-what-is-personal-data</id>
        <link href="https://your-docusaurus-test-site.com/gdpr-for-engineers-what-is-personal-data"/>
        <updated>2019-05-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[We all know that GDPR (also known as RGPD in France) has brought data policy into the spotlight for many technical organizations. As of May 25, 2018, if your systems (both automated and otherwise!) handle PII of individuals residing in the EU, you must comply with regulation. While this enforcement date makes the topic seem like old news, many US-based companies are unclear of the specifics and vastly underprepared to deal with the implications.]]></summary>
        <content type="html"><![CDATA[<p>We all know that GDPR (also known as RGPD in France) has brought data policy into the spotlight for many technical organizations. As of May 25, 2018, if your systems (both automated and otherwise!) handle PII of individuals residing in the EU, you must comply with regulation. While this enforcement date makes the topic seem like old news, many US-based companies are unclear of the specifics and vastly underprepared to deal with the implications.</p><p>Before diving too far into the deep end of implementation detail, one must first understand the basics. The only way to conform to this regulation (which many US-based companies still don't) is to thoroughly understand what data needs to be handled with care.</p><p>So... what is personal data?</p><p><strong>At its core, personal data is any information that relates to an identified or identifiable living individual. It also pertains to different pieces of information that, when collected together, can lead to the identification of an individual.</strong></p><p>But, what does this mean exactly from a practical perspective? Some of the following may be obvious, but some may not be. Let's dive in...</p><h1>PII</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="name--surname-is-personal-data">Name + Surname is personal data.<a class="hash-link" href="#name--surname-is-personal-data" title="Direct link to heading">​</a></h3><p>The name "George Washington" is PII. If your application includes user registration functionality or your systems process and ship orders to a named individual, you're dealing with PII.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="home-address-is-personal-data">Home address is personal data.<a class="hash-link" href="#home-address-is-personal-data" title="Direct link to heading">​</a></h3><p>Again, if you ship orders to a home address you're handling PII.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="an-email-address-if-it-includes-personal-information-like-firstlast-name-is-personal-data">An email address, if it includes personal information like first/last name, is personal data.<a class="hash-link" href="#an-email-address-if-it-includes-personal-information-like-firstlast-name-is-personal-data" title="Direct link to heading">​</a></h3><p><a href="mailto:georgewashington1792@hotmail.com" target="_blank" rel="noopener noreferrer">georgewashington1792@hotmail.com</a> is PII. Since there's no guarantee incoming email addresses will be inherently free of PII, it is a good idea to consider all email addresses PII.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="any-personal-identification-card-number-is-personal-data">Any personal identification card number is personal data.<a class="hash-link" href="#any-personal-identification-card-number-is-personal-data" title="Direct link to heading">​</a></h3><p>Bank account number, driver's license number, passport number, and social security number are all PII.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="location-data-device-information-from-a-phone-or-laptop-is-personal-data">Location data (device information from a phone or laptop) is personal data.<a class="hash-link" href="#location-data-device-information-from-a-phone-or-laptop-is-personal-data" title="Direct link to heading">​</a></h3><p>Incoming latitude and longitude data (which flows freely from mobile devices) is considered PII.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="ip-address-is-personal-data">IP address is personal data.<a class="hash-link" href="#ip-address-is-personal-data" title="Direct link to heading">​</a></h3><p>Client IP address is considered PII. If your systems log X-Forwarded-For or any form of the original client IP, your logs are filled with PII.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="various-cookies-can-be-personal-data">Various cookies can be personal data.<a class="hash-link" href="#various-cookies-can-be-personal-data" title="Direct link to heading">​</a></h3><p>If your application stores any form of PII in cookies, these cookies are also PII (and the transitive property of equality has struck again).</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="ad-identifiers-android-ios-devices-are-personal-data">Ad identifiers (android/ ios devices) are personal data.<a class="hash-link" href="#ad-identifiers-android-ios-devices-are-personal-data" title="Direct link to heading">​</a></h3><p>Android and iOS devices have device-specific advertising identifiers. These are considered PII.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="healthcare-data-is-usually-personal-data">Healthcare data is usually personal data.<a class="hash-link" href="#healthcare-data-is-usually-personal-data" title="Direct link to heading">​</a></h3><p>This one goes without saying.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="pseudonymized-data-that-can-be-used-in-conjunction-with-another-source-to-re-identify-is-pii">Pseudonymized data that can be used in conjunction with another source to re-identify is PII.<a class="hash-link" href="#pseudonymized-data-that-can-be-used-in-conjunction-with-another-source-to-re-identify-is-pii" title="Direct link to heading">​</a></h3><p>If you're pseudonymizing "email", but the output of the pseudonymization process can used alongside another source to tie back to the original individual, it's still PII.</p><h1>NOT PII</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="formerly-personal-data-that-has-been-rendered-completely-anonymous-is-no-longer-personal-data">Formerly-personal data that has been rendered completely anonymous is no longer personal data.<a class="hash-link" href="#formerly-personal-data-that-has-been-rendered-completely-anonymous-is-no-longer-personal-data" title="Direct link to heading">​</a></h3><p>If a piece of PII has been rendered completely anonymous (typically through hashing it), it is no longer considered personal data. MD5-ing data like email addresses or first/last names is a perfectly acceptable method of eliminating the PII designation, especially when it is combined with a salt.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="company-registration-identifiers-are-not-personal-data">Company registration identifiers are not personal data.<a class="hash-link" href="#company-registration-identifiers-are-not-personal-data" title="Direct link to heading">​</a></h3><p>Registration details used internally within a specific company to designate an individual are not considered PII.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="primary-keys-on-your-users-table-are-not-personal-data">Primary keys on your "users" table are not personal data.<a class="hash-link" href="#primary-keys-on-your-users-table-are-not-personal-data" title="Direct link to heading">​</a></h3><p>Database internals (like a primary key or unique user-specific <code>uuid</code> value) are not considered PII, and are an easy way to isolate PII without losing referential integrity.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="email-addresses-that-do-not-include-personal-information-are-not-personal-data">Email addresses that do not include personal information are not personal data.<a class="hash-link" href="#email-addresses-that-do-not-include-personal-information-are-not-personal-data" title="Direct link to heading">​</a></h3><p><a href="mailto:twinkylvr234@hotmail.com" target="_blank" rel="noopener noreferrer">twinkylvr234@hotmail.com</a> is (sadly) not considered PII. But again, it's a good ideal to treat all emails as PII.</p><h1>In conclusion</h1><p>PII is everywhere! GDPR has drastically changed the requirements for data processing and storage of personal data, but without knowing what "personal details" are exactly it is very hard to comply with regulation. If your application collects, processes, or stores personal data of European residents, you're responsible for knowing and complying with the laws.</p>]]></content>
        <author>
            <name>Jake</name>
        </author>
        <category label="gdpr" term="gdpr"/>
        <category label="engineers" term="engineers"/>
        <category label="data privacy" term="data privacy"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Client-side instrumentation for under $1 per month. No servers necessary.]]></title>
        <id>client-side-instrumentation-for-under-one-dollar</id>
        <link href="https://your-docusaurus-test-site.com/client-side-instrumentation-for-under-one-dollar"/>
        <updated>2018-11-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In a world where the importance of data is steadily increasing yet the cost of computing power is steadily decreasing, there are fewer and fewer excuses to not have control of your own data. To explore that point I instrumented this site as inexpensively as I possibly could, without sacrificing reliability or functionality. I have full control of all data that is generated, the instrumentation is highly customizable, the output is simple to use, and I don't have to be available at all hours to keep it working.]]></summary>
        <content type="html"><![CDATA[<p>In a world where the importance of data is steadily increasing yet the cost of computing power is steadily decreasing, there are fewer and fewer excuses to not have control of your own data. To explore that point I instrumented this site as inexpensively as I possibly could, without sacrificing reliability or functionality. I have full control of all data that is generated, the instrumentation is highly customizable, the output is simple to use, and I don't have to be available at all hours to keep it working.</p><p>The components utilized include Cloudfront, S3, Lambda, Athena, and the Snowplow Javascript tracker.</p><p><strong>And It costs less than $1 per month.</strong></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="before-getting-started-what-does-the-system-need">Before getting started.... what does the system need?<a class="hash-link" href="#before-getting-started-what-does-the-system-need" title="Direct link to heading">​</a></h3><ul><li>Easy javascript tracking integration, with no impact on site performance.</li><li>Absolute minimization of resources for me to manage, pay for, and worry about.</li><li>Rock-solid pipeline reliability.</li></ul><p>So let's get started...</p><h1>No servers.</h1><p>A good stream-based analytics infrastructure is a beautiful thing when it's done correctly, but there's always overhead involved. You'll need streams, you'll need collection servers sitting behind a load balancer/reverse proxy, you'll need machines solely responsible for enrichment, you'll need infrastructure for monitoring infrastructure, you'll need engineers who demand salaries, and.... you get the idea.</p><p>For this site I don't have any intention of modifying functionality or making decisions based on what visitors are doing "right now", so stream-based infrastructure just introduces overhead and is out of the question. Millisecond-level latency to the eventual data store (like in a stream-based approach) would be nice, but minute-level latency is perfectly ok.</p><h1>No monitoring.</h1><p>Blasphemy! Or... maybe not. Instead of setting up lots of infrastructure that must be continually monitored, all instrumentation infrastructure leans heavily on AWS functionality. By taking advantage of various AWS built-ins (with a high level of reliability), I was able to skip a lot of overhead that would otherwise be required.</p><p>Cloudfront includes the ability to forward access logs to an S3 bucket.</p><p>Lambda triggers ensure at-least-once notification of a predefined event (like when a file lands in S3).</p><p>S3 provides 99.999999999% durability and 99.99% availability on a yearly SLA.</p><h1>No fees (almost).</h1><p>The current cost of this system is fairly satisfying, but my favorite part is the fact that site traffic would have to increase significantly before cost increases beyond what I'm willing to pay.</p><p>Cloudfront costs ~$0.085 per GB out to the internet, ~$0.020 per GB out to origin, and $0.0075 per 10,000 requests.</p><p><strong>Total Cloudfront cost this month: $0.03</strong></p><p>Lambda is free up to 1M requests and 3.2M seconds of compute time, per month.</p><p><strong>Total Lambda cost this month: $0.00</strong></p><p>S3 costs $0.023 per GB, per month, $0.01 per 1,000 PUT/COPY/POST requests, and $0.001 per 1,000 GET/SELECT requests.</p><p><strong>Total S3 cost this month: $0.27</strong></p><p>Athena costs $5 per TB of data scanned.</p><p><strong>Total Athena cost this month: $0.01</strong></p><h1>Quick and easy data access.</h1><p>As data volume grows, I need to be able to query all data quickly and effectively in a primarily-ad-hoc fashion. AWS Athena is effectively presto-as-a-service, and an external table backed by S3 was a straight-forward, inexpensive way forward.</p><p>Putting a visualization layer like Quicksight on top of Athena is very simple.</p><h1>How it is implemented.</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="step-1---aws-cloudfront-distribution">Step #1 - AWS Cloudfront Distribution<a class="hash-link" href="#step-1---aws-cloudfront-distribution" title="Direct link to heading">​</a></h3><p>The first step of setting up instrumentation requires creating two S3 buckets and a Cloudfront distribution. I've automated the setup with Terraform, and the block of code for doing so (not including tfvars/etc) looks like the following:</p><div class="codeBlockContainer_MPoW theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_B9tL"><pre tabindex="0" class="prism-code language-text codeBlock__0OG thin-scrollbar"><code class="codeBlockLines_gEuF"><span class="token-line" style="color:#393A34"><span class="token plain">resource "aws_s3_bucket" "src" {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  bucket = "${var.env}-${var.owner}-${var.system_tag}-lt-src"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  acl    = "public-read"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tags {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Name   = "${var.env}-${var.owner}-${var.system_tag}-lt-src"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Env    = "${var.env}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    System = "${var.system_tag}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Owner  = "${var.owner}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">resource "aws_s3_bucket_object" "object" {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  bucket       = "${aws_s3_bucket.src.bucket}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  key          = "i"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  source       = "${"${path.module}/files/i"}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  etag         = "${md5(file("${path.module}/files/i"))}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  acl          = "public-read"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  content_type = "image/gif"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">resource "aws_s3_bucket" "logs" {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  bucket = "${var.env}-${var.owner}-${var.system_tag}-lt-logs"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  acl    = "private"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tags {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Name   = "${var.env}-${var.owner}-${var.system_tag}-lt-logs"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Env    = "${var.env}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    System = "${var.system_tag}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Owner  = "${var.owner}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">resource "aws_cloudfront_distribution" "log_distribution" {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  origin {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    domain_name = "${aws_s3_bucket.src.bucket_regional_domain_name}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    origin_id   = "S3-${aws_s3_bucket.src.bucket}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  enabled         = true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  is_ipv6_enabled = true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  comment         = "Cloudfront distribution for snowplow tracking pixel"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  logging_config {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    include_cookies = true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    bucket          = "${aws_s3_bucket.logs.bucket_regional_domain_name}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    prefix          = "RAW"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  aliases = ["sp.${var.env}.${var.primary_domain}"]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  default_cache_behavior {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    allowed_methods  = ["GET", "HEAD"]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cached_methods   = ["GET", "HEAD"]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    target_origin_id = "S3-${aws_s3_bucket.src.bucket}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    forwarded_values {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      query_string = true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      cookies {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        forward = "all"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    viewer_protocol_policy = "allow-all"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    min_ttl                = 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    default_ttl            = 3600</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max_ttl                = 86400</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  restrictions {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    geo_restriction {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      restriction_type = "whitelist"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      locations        = ["AF", "AX", "AL", "DZ", "AS", "AD", "AO", "AI", "AQ", "AG", "AR", "AM", "AW", "AU", "AT", "AZ", "BS", "BH", "BD", "BB", "BY", "BE", "BZ", "BJ", "BM", "BT", "BO", "BA", "BW", "BV", "BR", "IO", "BN", "BG", "BF", "BI", "CV", "KH", "CM", "CA", "KY", "CF", "TD", "CL", "CN", "CX", "CC", "CO", "KM", "CG", "CD", "CK", "CR", "CI", "HR", "CU", "CW", "CY", "CZ", "DK", "DJ", "DM", "DO", "EC", "EG", "SV", "GQ", "ER", "EE", "SZ", "ET", "FK", "FO", "FJ", "FI", "FR", "GF", "PF", "TF", "GA", "GM", "GE", "DE", "GH", "GI", "GR", "GL", "GD", "GP", "GU", "GT", "GG", "GN", "GW", "GY", "HT", "HM", "VA", "HN", "HK", "HU", "IS", "IN", "ID", "IR", "IQ", "IE", "IM", "IL", "IT", "JM", "JP", "JE", "JO", "KZ", "KE", "KI", "KP", "KR", "KW", "KG", "LA", "LV", "LB", "LS", "LR", "LY", "LI", "LT", "LU", "MO", "MK", "MG", "MW", "MY", "MV", "ML", "MT", "MH", "MQ", "MR", "MU", "YT", "MX", "FM", "MD", "MC", "MN", "ME", "MS", "MA", "MZ", "MM", "NA", "NR", "NP", "NL", "NC", "NZ", "NI", "NE", "NG", "NU", "NF", "MP", "NO", "OM", "PK", "PW", "PA", "PG", "PY", "PE", "PH", "PN", "PL", "PT", "PR", "QA", "RE", "RO", "RU", "RW", "BL", "KN", "LC", "MF", "PM", "VC", "WS", "SM", "ST", "SA", "SN", "RS", "SC", "SL", "SG", "SX", "SK", "SI", "SB", "SO", "ZA", "GS", "SS", "ES", "LK", "SD", "SR", "SJ", "SE", "CH", "SY", "TJ", "TH", "TL", "TG", "TK", "TO", "TT", "TN", "TR", "TM", "TC", "TV", "UG", "UA", "AE", "GB", "US", "UM", "UY", "UZ", "VU", "VE", "VN", "VG", "VI", "WF", "EH", "YE", "ZM", "ZW"]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tags {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Name   = "${var.env}-${var.owner}-${var.system_tag}-log-dist"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Env    = "${var.env}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    System = "${var.system_tag}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Owner  = "${var.owner}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  viewer_certificate {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cloudfront_default_certificate = true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup_hRr1"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>This Cloudfront distribution serves a gif titled "i" from S3, which becomes the tracking endpoint. All access logs are then forwarded to another S3 bucket where they are subsequently processed.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="step-2---snowplow-javascript-tracker">Step #2 - Snowplow Javascript Tracker<a class="hash-link" href="#step-2---snowplow-javascript-tracker" title="Direct link to heading">​</a></h3><p>I am a huge fan of Snowplow. Integration is easy, functionality is rich, fault tolerance is built-in, and I know from experience that the system works very, very well.</p><p>Page views, page pings, link clicks, and forms are tracked by default. I've also added custom structured events for tracking specific actions that are otherwise uninstrumented.</p><p>The Javascript tracking code gets included in the <code>&lt;head&gt;</code> of every page, and looks like the following:</p><div class="codeBlockContainer_MPoW theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_B9tL"><pre tabindex="0" class="prism-code language-text codeBlock__0OG thin-scrollbar"><code class="codeBlockLines_gEuF"><span class="token-line" style="color:#393A34"><span class="token plain">&lt;script type="text/javascript"&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ;(function(p,l,o,w,i,n,g){if(!p[i]){p.GlobalSnowplowNamespace=p.GlobalSnowplowNamespace||[];</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  p.GlobalSnowplowNamespace.push(i);p[i]=function(){(p[i].q=p[i].q||[]).push(arguments)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  };p[i].q=p[i].q||[];n=l.createElement(o);g=l.getElementsByTagName(o)[0];n.async=1;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  n.src=w;g.parentNode.insertBefore(n,g)}}(window,document,"script","//d1fc8wv8zag5ca.cloudfront.net/2.6.2/sp.js","snowplow"));</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  window.snowplow('newTracker', 'cf', 'dolaqvbw76wrx.cloudfront.net', {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    appId: 'site',</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cookieDomain: 'bostata.com',</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  });</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  window.snowplow('enableActivityTracking', 1, 5);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  window.snowplow('trackPageView');</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  window.snowplow('enableLinkClickTracking');</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  window.snowplow('enableFormTracking');</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;/script&gt;</span><br></span></code></pre><div class="buttonGroup_hRr1"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="step-3---aws-lambda">Step #3 - AWS Lambda<a class="hash-link" href="#step-3---aws-lambda" title="Direct link to heading">​</a></h3><p>The next aspect of this infrastructure is a Lambda function that listens to the Cloudfront distribution log bucket for any ObjectCreated event, processes all incoming access logs, appends rich context to them (geoip, useragent parser, query parameter parsing, etc), formats these records in a consistent manner to the Snowplow canonical event model, and persists formatted records back to the log bucket as gzipped json files.</p><p>Up to 1000 concurrent instances of this function can be running at any time, and each instance is limited to 128mb of memory.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="step-4---aws-athena">Step #4 - AWS Athena<a class="hash-link" href="#step-4---aws-athena" title="Direct link to heading">​</a></h3><p>It's fun to watch data pile up in S3, but the value of said data is very low if it's difficult to access. Athena allows you to query S3 buckets quickly and easily via external tables... and the price is right. Creating a table for the Snowplow canonical event model is pretty simple:</p><div class="codeBlockContainer_MPoW theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_B9tL"><pre tabindex="0" class="prism-code language-text codeBlock__0OG thin-scrollbar"><code class="codeBlockLines_gEuF"><span class="token-line" style="color:#393A34"><span class="token plain">CREATE EXTERNAL TABLE default.events</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  app_id string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  platform string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  etl_tstamp timestamp,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  collector_tstamp timestamp,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dvce_created_tstamp timestamp,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  event string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  event_id string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  txn_id string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  name_tracker string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  v_tracker string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  v_collector string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  v_etl string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  user_id string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  user_ipaddress string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  user_fingerprint string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  domain_userid string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  domain_sessionidx string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  network_userid string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  geo_country string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  geo_region string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  geo_city string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  geo_zipcode string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  geo_latitude string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  geo_longitude string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  geo_region_name string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ip_isp string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ip_organization string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ip_domain string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ip_netspeed string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  page_url string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  page_title string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  page_referrer string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  page_urlscheme string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  page_urlhost string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  page_urlport string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  page_urlpath string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  page_urlquery string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  page_urlfragment string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  refr_urlscheme string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  refr_urlhost string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  refr_urlport string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  refr_urlpath string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  refr_urlquery string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  refr_urlfragment string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  refr_medium string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  refr_source string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  refr_term string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  mkt_medium string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  mkt_source string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  mkt_term string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  mkt_content string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  mkt_campaign string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  contexts string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  se_category string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  se_action string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  se_label string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  se_property string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  se_value string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  unstruct_event string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tr_orderid string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tr_affiliation string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tr_total string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tr_tax string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tr_shipping string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tr_city string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tr_state string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tr_country string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ti_orderid string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ti_sku string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ti_name string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ti_category string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ti_price string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ti_quantity string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  pp_xoffset_min string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  pp_xoffset_max string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  pp_yoffset_min string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  pp_yoffset_max string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  useragent string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_name string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_family string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_version string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_type string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_renderengine string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_lang string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_features_pdf string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_features_flash string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_features_java string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_features_director string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_features_quicktime string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_features_realplayer string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_features_windowsmedia string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_features_gears string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_features_silverlight string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_cookies string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_colordepth string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_viewwidth string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  br_viewheight string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  os_name string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  os_family string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  os_manufacturer string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  os_timezone string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dvce_type string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dvce_ismobile string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dvce_screenwidth string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dvce_screenheight string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  doc_charset string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  doc_width string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  doc_height string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tr_currency string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tr_total_base string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tr_tax_base string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tr_shipping_base string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ti_currency string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ti_price_base string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  base_currency string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  geo_timezone string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  mkt_clickid string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  mkt_network string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  etl_tags string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  dvce_sent_tstamp timestamp,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  refr_domain_userid string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  refr_dvce_tstamp timestamp,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  derived_contexts string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  domain_sessionid string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  derived_tstamp timestamp,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  event_vendor string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  event_name string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  event_format string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  event_version string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  event_fingerprint string,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  file_name string</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ROW FORMAT  serde 'org.apache.hive.hcatalog.data.JsonSerDe'</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">LOCATION    's3://prod-logs-bucket/PROCESSED';</span><br></span></code></pre><div class="buttonGroup_hRr1"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h1>See it at work</h1><p>Since it's always fun to see a system working in the wild, let's watch exactly how this instrumentation works. I'm using the developer tools on Chrome to inspect browser network traffic.</p><p>Each time a page is loaded, the browser requests a sp.js file that's cached in Cloudfront as seen here:</p><p><img loading="lazy" alt="spjs" src="/assets/images/spjs-e9a298be1bf3e45044bffa371484be29.png" width="534" height="469" class="img_E7b_"></p><p>Immediately following that, a page view event gets fired to the Cloudfront distribution that was set up above:</p><p><img loading="lazy" alt="pageview" src="/assets/images/02_pageview-1c7c0f1744828b1430700f66b20932af.png" width="719" height="699" class="img_E7b_"></p><p>A page ping event fires to the Cloudfront distribution every five seconds, and indicates that I'm still actively reading or scrolling:</p><p><img loading="lazy" alt="pageping" src="/assets/images/03_pageping-e213a3c7aae323e08b973322754215d8.png" width="720" height="379" class="img_E7b_"></p><p>With all of this wonderful tracking, the Lambda function will start going to work:</p><p><img loading="lazy" alt="lambda" src="/assets/images/04_lambda-f023822fad030e760c1db1d9fac35d24.png" width="720" height="442" class="img_E7b_"></p><p>And by the time you've reached this point, the data will be readily accessible in S3 and queryable via Athena:</p><p><img loading="lazy" alt="athena" src="/assets/images/athena-f43e6e0612d4595fd231e2027ac9f30f.png" width="720" height="418" class="img_E7b_"></p><h1>In conclusion</h1><p>Overall, I'd say this exploration has been a big win. Client-side site instrumentation typically requires significant setup and maintenance overhead, but this methodology is the exact opposite. It's cheap. It's fast. It's quick to set up. It's reliable. It's flexible. And it just works.</p><p>The price of system operation will continue to be minimal, the data stays close-to-home, and I don't have to think about data pipeline issues.</p><p>And all for under $1 per month.</p>]]></content>
        <author>
            <name>Jake</name>
        </author>
        <category label="snowplow" term="snowplow"/>
        <category label="serverless" term="serverless"/>
        <category label="instrumentation" term="instrumentation"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Your Company Should Own Its Own Data]]></title>
        <id>why-your-company-should-own-its-own-data</id>
        <link href="https://your-docusaurus-test-site.com/why-your-company-should-own-its-own-data"/>
        <updated>2018-10-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[When considering software and related infrastructure, the business of today is caught in a never-ending cycle of "build vs. buy". Many third-party companies solve serious challenges such as managing sales pipelines, accounting automation, payment processing, and internal communication. These alternatives to "building it yourself" empower companies to operate faster or more efficiently, and overall benefit to the customer is often net-positive. When considering various alternatives, there is one critical component of your business that you should strongly reconsider leaving in the hands of third parties, however: your data and supporting data infrastructure.]]></summary>
        <content type="html"><![CDATA[<p>When considering software and related infrastructure, the business of today is caught in a never-ending cycle of "build vs. buy". Many third-party companies solve serious challenges such as managing sales pipelines, accounting automation, payment processing, and internal communication. These alternatives to "building it yourself" empower companies to operate faster or more efficiently, and overall benefit to the customer is often net-positive. When considering various alternatives, there is one critical component of your business that you should strongly reconsider leaving in the hands of third parties, however: your data and supporting data infrastructure.</p><p>The most progressive companies in the world, of all industries and sizes, have one thing in common: an obsession with the collection and in-house ownership of data. Why invest in your own data infrastructure? It adds long-term value to your organization, has positive financial implications, and gives your company competitive advantages that are achievable in no other way.</p><h1>Owning your data adds to your company's long-term value.</h1><p>Imagine you're an investor, evaluating two functionally-equivalent companies. Both companies have similar technology, a large customer base, provide real value to the world, and have been in business for about five years. Company 1 has four years' worth of site traffic and customer behavior, application performance metrics, clean financial data, git (code change) history, and a number of external datasets relevant to its core competencies. Company 2 simply has its cofounders and a small subset of the engineering it has employed along the way.</p><p><strong>Which one would you invest in?</strong></p><p><em>Probably not the one with minimal data to back their claims up.</em></p><p>Having full ownership of your own data makes tracking unsampled site performance over time easy. With a product like Google Analytics, on the other hand, that is impossible to do unless you are held hostage at an annual cost of over $150,000. Want to definitively prove business growth or product efficacy? Want to prove your systems have become more efficient or technical competence has increased throughout your company's existence? Instrument your systems and warehouse the data.</p><p>It is only through in-house ownership of your data that this is possible.</p><h1>Owning your data has large financial implications- both now and in the future.</h1><p>Let's consider a widely-used system for site instrumentation and analtics previously mentioned: Google Analytics. If you want a "pure", unsampled view of your site traffic you will need to become a Google Analytics Premium (now branded "360") customer at a cost of over $150,000 annually. What happens when you become a paying customer, only to decide five years down the road that you need to cut costs and downgrade the service? All your historical data will be sampled, and you will be left with only a rough estimate of site traffic over time. You certainly wouldn't want to lose insight you once had, and are therefore held hostage by your own data.</p><p>Let's consider another commonly-used platform for application instrumentation: Mixpanel. Mixpanel is incredibly easy to set up, straight-forward to add custom instrumentation, and initially inexpensive. But as your site scales or you want to instrument more and more components, the cost of the service becomes prohibitively expensive. Whether or not your revenue scales relative to site traffic, the cost to you certainly will. At a rate that is typically higher than your business itself is scaling. Again, you don't want to lose historical context of your business by downgrading instrumentation, so you become held hostage by the very data your application generated.</p><h1>Owning your data gives you competitive advantages that are achievable in no other way.</h1><p>When your site scales yet the associated data collection systems are owned by another company (and rented by yours), who has the advantage? The answer is simple: not you!</p><p>What happens when you stop paying a third-party service?</p><p>You lose access, and they keep your data. If they're lucky, you forgot about the tracking code placed on your site, and continue to ship them data for an extended period of time.</p><p>What happens when Google wants to launch a product that directly competes with yours?</p><p>They have years of your site traffic data, and probably your closest competitors' data as well.</p><p>What happens when the data your systems or customers produced has insight that can be monetized, or activity that can be aggregated, enriched, and sold to a partner?</p><p>Your company won't be seeing any of those financial benefits, sorry!</p><p>Conversely...</p><h1>If you own your data, you have full control.</h1><p>Want to track activity at a finer granularity than you can with an existing tool? You can do that, because it's yours!</p><p>Want to ensure you comply with GDPR?</p><p>It's much easier if you control the collection and persistence of personal data, and can simply issue a <code>DELETE FROM some_table WHERE id = 10;</code> sql statement!</p><p>Want to persist terabytes of data in a 99.999999999% durable location that is accessible at any time? You can do so with AWS S3 at a whopping price of $0.023 per GB, per month. Have more data than that, and want to roll it into long-term cold storage? AWS Glacier exists for exactly that reason, and is priced at a convenient $0.004 per GB, per month. Thanks to Moore's law, data storage gets less expensive over time, unlike any third-party data service provider on the market.</p><h1>Conclusion</h1><p>In order to build the most value possible for your organization, realize maximum financial benefits, and retain various competitive advantages, you should own your data. While many third-party data services will be easy to set up initially, your business will probably find itself with maximum expenditures, minimum flexibility, lacking ownership of years' of data it has created, and a lot of hassle long term. In order to maintain maximum competitive advantage, future flexibility, and overall cost efficiencies, you must own and manage your own data.</p>]]></content>
        <author>
            <name>Jake</name>
        </author>
        <category label="data" term="data"/>
        <category label="ownership" term="ownership"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Pipeline Design Considerations]]></title>
        <id>data-pipeline-design-considerations</id>
        <link href="https://your-docusaurus-test-site.com/data-pipeline-design-considerations"/>
        <updated>2018-02-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[There are many factors to consider when designing data pipelines, which include disparate data sources, dependency management, interprocess monitoring, quality control, maintainability, and timeliness. Toolset choices for each step are incredibly important, and early decisions have tremendous implications on future successes. The following post is meant to be a reference to ask the right questions from the start of the design process, instead of halfway through. In terms of the V-Model of systems engineering, it is intended to fall between the “high level design” and “detailed design” steps:]]></summary>
        <content type="html"><![CDATA[<p>There are many factors to consider when designing data pipelines, which include disparate data sources, dependency management, interprocess monitoring, quality control, maintainability, and timeliness. Toolset choices for each step are incredibly important, and early decisions have tremendous implications on future successes. The following post is meant to be a reference to ask the right questions from the start of the design process, instead of halfway through. In terms of the V-Model of systems engineering, it is intended to fall between the “high level design” and “detailed design” steps:</p><p><img loading="lazy" alt="v-curve" src="/assets/images/v-curve-fcddea49428eb9920bb7c910e69d8bc6.png" width="1348" height="754" class="img_E7b_"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="preliminary-considerations">Preliminary Considerations<a class="hash-link" href="#preliminary-considerations" title="Direct link to heading">​</a></h2><p>Before selecting toolsets, and certainly before writing any code, there are many subsystem factors and expectations to be taken into account. When designing production-facing data pipelines, you will need to take these factors into consideration.</p><h1>Data: Origin, Type, and Timeliness</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="origin">Origin<a class="hash-link" href="#origin" title="Direct link to heading">​</a></h3><p>First and foremost, the origin of the data in question must be well understood, and that understanding must be shared across engineers to minimize downstream inconsistencies. Assumptions concerning data structure and interpretation are very hard to work around once they are baked into reports and/or managerial decisions, so it’s incredibly important to get this step right.</p><p>Secondly, an investigation into how to get data from the production application must be performed. Can application data be queried/ exported from the production database, in bulk, without detrimentally affecting the user experience? Are you sure about that? Spoiler alert: you'll probably want to set up replication on the production database before the export process is hardened.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="type">Type<a class="hash-link" href="#type" title="Direct link to heading">​</a></h3><p>The type of data involved is another important aspect of system design, and data typically falls into one of two categories: event-based and entity data. Event-based data is denormalized, and is used to describe actions over time, while entity data is normalized (in a relational db, that is) and describes the state of an entity at the current point in time. In Ralph Kimball’s data warehousing terminology, event-based data corresponds to facts while entity data corresponds to dimensions.</p><p>At first glance, event-based data lends itself to incremental ingestion via high-water marks, while entity data lends itself to bulk ingestion or change data capture. If the normalized data model includes a modified_at (or equivalent) column on entity tables, and it is trustworthy, various entity data can also be ingested incrementally to relieve unnecessary load. Ideally, data should always be incrementally ingested and processed, but reality says that is not always an option.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="timeliness">Timeliness<a class="hash-link" href="#timeliness" title="Direct link to heading">​</a></h3><p>How quickly must data be gathered from the production system, and how quickly must it be processed? Data pipelining methodologies will vary widely depending on the desired speed of data ingestion and processing, so this is a very important question to answer prior to building the system. Ideally, event-based data should be ingested almost instantaneously to when it is generated, while entity data can either be ingested incrementally (ideally) or in bulk. If all data ingestion processes are incremental, making the process faster is simply just a matter of running the particular job more often. Therefore, that should be the goal.</p><p>Kafka is a very good option for realtime website activity tracking as it was created by Linkedin to do exactly that. It offers the ability for messages to be replayed, incorporates extensive fault-tolerance, can be partitioned, etc. RabbitMQ and Snowplow are other very suitable options, and solve similar problems in slightly different ways.</p><p>For pulling data in bulk from various production systems, toolset choices vary widely, depending on what technologies are implemented at the source. In Postgresql, these choices include <code>COPY (some_query) TO STDOUT WITH CSV HEADER</code>, a dblink from one database to another, streaming replication via the write-ahead log, or using a <code>pg_dump --table sometable --no-privileges | some_file.sql</code> script. Redshift's functionality is very similar, but the system uses S3 as an intermediary to <code>UNLOAD ('some_query') TO 's3://bucket'</code>.</p><h1>Storage Mechanisms</h1><p>When it comes to choosing a storage mechanism, the largest factors to be considered include the volume of data and the query-ability of said data (if "query-ability" is indeed a word). If a limited amount of volume is expected, or data is pre-aggregated elsewhere, many storage options will suffice. If incoming data is to be collected in sufficiently large volume or if the storage mechanism must allow for downstream exploratory querying, storage options decrease significantly. If queries are defined beforehand and the volume of data is the limiting factor, Hadoop is a solid alternative. If a high volume of data is to be collected but will be queried in an exploratory way, Redshift is a better alternative. The parity to older versions of Postgres (8.0.2) and the fact that the surface looks/ feels like a regular Postgres database make it very easy to learn and utilize.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="questions-to-ask-when-choosing-a-storage-mechanism">Questions to ask when choosing a storage mechanism<a class="hash-link" href="#questions-to-ask-when-choosing-a-storage-mechanism" title="Direct link to heading">​</a></h3><ul><li>What level of volume is expected? 200M rows in a single table makes Postgres crawl, especially if it isn’t partitioned. If it is partitioned, queries must be altered accordingly to avoid scanning each partition.</li><li>Will data be exploratorily queried, or are queries defined already and will be semi-static in the future? Tool options and distribution/ sorting strategies will need to be altered accordingly.</li><li>What level of maintenance do you wish to perform? Ideally, engineering resources will go to setting up the system, and future maintenance will be minimal and/or only performed as absolutely necessary.</li><li>Will dashboarding/ analysis tools be pointed at the raw data, or will data be aggregated and moved elsewhere?</li><li>Who will be accessing the data? A system like Hadoop is not ideal for junior analysts with limited SQL/bash knowledge.</li></ul><h1>Language Selection</h1><p>For the sake of developmental speed, maintainability, and parity with various pipelining tools, Python is a very solid language selection. The key component to this is consistency across the system - especially when the team is small. Usually the “Python is slow” argument can be overcome by Python’s multiprocessing or concurrent.futures modules, or by only processing as much as is absolutely necessary. Or by waiting for Intel to optimize for Python. But that's a different story.</p><p>Almost every single data pipelining tool has a Python client, and the language allows for future flexibility while maintaining speed of initial development. It also allows for rapid onboarding of new developers, efficient testing with mock, remote bash execution/deployment with fabric, and much more. Companies that continually process enormous amounts of data use Python extensively, and there’s a reason they have selected it as language of choice. Lastly, Airbnb’s Airflow and Spotify’s Luigi are both conveniently written in Python.</p><h1>ETL Dependency Management</h1><p>Dependency management is easy to conceptualize, but a little more difficult to manage across multiple worker nodes or <em>cough</em> a Docker Swarm. This is where tools like Luigi, Airflow, and even Jenkins for remote execution scheduling come into play.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="time-based-scheduling">Time-based scheduling<a class="hash-link" href="#time-based-scheduling" title="Direct link to heading">​</a></h3><p>In short, pipeline jobs should never be executed via time-based scheduling. If a one-hour job that is scheduled at 8am fails, all downstream jobs should be aware of that failure and downstream execution should be modified accordingly. If the job starts taking longer than one hour due to increased volume or processing requirements, downstream jobs should be aware and only execute after the upstream job has successfully run. Time-based scheduling cannot efficiently handle either condition, and requires engineers continuously modify the schedule. This wastes time, and is completely unnecessary.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="single-vs-multiple-dependencies">Single vs. multiple dependencies<a class="hash-link" href="#single-vs-multiple-dependencies" title="Direct link to heading">​</a></h3><p>If an ETL job only has one upstream dependency, Jenkins is a perfectly suitable tool for linking jobs together. It is quite straight-forward to set up, allows for remote execution on multiple nodes, and is simply a matter of passing the necessary bash script to the job configuration. It also allows for one job to kick off multiple downstream tasks after execution (ie, “load the data you just aggregated to a foreign database, and let the world know it’s happening”).</p><p>If an ETL job has multiple upstream dependencies, Jenkins becomes pretty clumsy. It is certainly possible to do so, but it’s not exactly pretty. Luigi and Airflow shine in this respect, because both are built to handle many upstream dependencies for a single job (DAG style).</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="visualizing-dependencies">Visualizing Dependencies<a class="hash-link" href="#visualizing-dependencies" title="Direct link to heading">​</a></h3><p>Jenkins doesn’t even try to visualize the acyclic graph of nested dependencies, while Luigi and Airflow both do. Visualizing the relationships between interconnected jobs is a huge time saver, and allows for the system to grow easily without overloading the ones who built it (and therefore slowing down future development).</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="separation-of-concerns">Separation of Concerns<a class="hash-link" href="#separation-of-concerns" title="Direct link to heading">​</a></h3><p>Jenkins is exceptional at job scheduling and handling a limited set of single dependencies, and that is all. Luigi is extremely good at multiple dependency handling and visualization, but it doesn’t even attempt to handle scheduling execution of the initial job in the acyclic graph. Airflow tries to do everything including job duration monitoring, plotting job execution overlap via Gantt charts, scheduling, and dependency management. In my opinion (I warned you I was opinionated), job durations and overlap are ideally tracked and handled elsewhere, alongside other pipeline instrumentation.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="balancing-dependency-management-with-job-overload">Balancing dependency management with job overload<a class="hash-link" href="#balancing-dependency-management-with-job-overload" title="Direct link to heading">​</a></h3><p>If a job dependency tool is used, every minuscule item of the ETL process should be not wrapped in a task. For example, if ten tables are to be exported from a remote database, and all must be exported before downstream tasks run, there should not be an import job for each of the ten tables. There should be one job that imports all the designated tables. This allows for the dependency graph to remain clean, code to be well-written and easily maintainable, etc.</p><h1>Fault Tolerance</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="persistence-in-message-based-ingestion">Persistence in message-based ingestion<a class="hash-link" href="#persistence-in-message-based-ingestion" title="Direct link to heading">​</a></h3><p>If incoming event data is message-based, a key aspect of system design centers around the inability to lose messages in transit, regardless of what point the ingestion system is in. Message queues with delivery guarantees are very useful for doing this, since a consumer process can crash and burn without losing data and without bringing down the message producer. Alternatively, the producer process can crash and burn, and the consumer will see nothing but the fact that no new messages have come in. Messages in transit should always be persisted to disk (if space and time allows) so that if the broker/queue goes down, it can be brought back up without losing data.</p><p>If done right, this method of data ingestion is extremely fault-tolerant and scalable. There are some key differences between various pub/sub systems (persistence, replayability, distributed commit log vs. queue), but that is a separate conversation.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="stop-on-error-in-bulk-processing">Stop-on-error in bulk processing<a class="hash-link" href="#stop-on-error-in-bulk-processing" title="Direct link to heading">​</a></h3><p>Fault tolerance is a key aspect of any data pipeline, and downstream tasks should always be aware of upstream tasks failing. Data pipelining is one place where exceptions should not always be handled in code, and engineers should know about any error in the system immediately. The main place exceptions should be handled is when retrying a task for a designated period of time (or number of retries/ exponential back-off). This is so that downstream jobs don’t run and mistakenly cause additional harm to data quality. The system should stop immediately when a fault is detected if downstream jobs depend on it.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="idempotence-and-replayability">Idempotence and replayability<a class="hash-link" href="#idempotence-and-replayability" title="Direct link to heading">​</a></h3><p>Bulk pipeline jobs should always be created so that they are able to be re-run immediately in case of failure, and entirely idempotent. No matter how many times a particular job is run, it should always produce the same output with a given input, and should not persist duplicate data to the destination. Want math?</p><p><code>f(f(x)) = f(x)</code></p><h1>Architectural separation of concerns</h1><p>In short, a production web application should never be dependent on a reporting database and/or data warehouse. This creates an unnecessary dependency which is inflexible for maintenance and increases the level of risk.</p><h1>Pipeline Monitoring</h1><p>As with any system, individual steps should be extensively instrumented and monitored. If they are not, it is nearly impossible to eliminate personal opinion and accurately determine the facts of system operation. A good starting point is to measure the time which a particular job started, stopped, total runtime, state of completion, and any pertinent error messages.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="start-and-stop-time">Start and stop time<a class="hash-link" href="#start-and-stop-time" title="Direct link to heading">​</a></h3><p>This one is pretty straight-forward, as the difference is used to calculate runtime. Logging start time of jobs is very useful on its own, however, since it adds a linear aspect to job execution (even if the jobs are executed in parallel). When debugging a step of the pipeline, it’s very helpful to see when the jobs were executed, in order by which they were kicked off. It also allows for easy visualization if need be.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="runtime">Runtime<a class="hash-link" href="#runtime" title="Direct link to heading">​</a></h3><p>This metric is also obvious yet incredibly powerful for determining the current state of the world, proactive system improvements, and early warning signs for future problems. The following insight is realized, simply by measuring runtime:</p><ul><li>Determination of system bottlenecks</li><li>Early-warning signs for jobs trending upwards, relative to other jobs</li><li>Statistically-benchmarked improvements</li><li>Determination of the effects on a particular system from external variables (server, network, etc)</li><li>Job completion and error logging</li></ul><p>Pipeline instrumentation not only allows for efficient debugging, early warning signs, and proactive improvements to the system, but it also allows the engineers to see progress over time. It also can be very easily used to expose various system statistics to outside parties, so that system scale can be easily and effectively communicated.</p><h1>Pipeline Monitoring Tools</h1><p>Monitoring each portion of the pipeline, as well as aggregate statistics, can be done in a variety of ways. Graphite was built to handle this type of data, but even a table that stores system metrics is much better than nothing all all. The combination of instrumentation decorators (in Python at least) and various signals being sent to Graphite is ideal. InfluxDB is another way to build instrumentation directly into your data pipelines.</p><p>For system instrumentation visualizations, I highly recommend Grafana.</p><h1>Accessibility and Visualization</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="tools">Tools<a class="hash-link" href="#tools" title="Direct link to heading">​</a></h3><p>There are many tools out there for accessing and visualizing data. For data exploration and team collaboration, tools like Wagon are great. For dashboarding and sharing data, Periscope is a good choice. When selecting a tool, it is ideal if the code involved is not proprietary to that particular tool. That is, the entire company shouldn’t be bound to a tool, simply because it uses a sql variant that is too painful to rewrite. Another key consideration is the ease of use for non-technical or semi-technical people. In order to proliferate a data-centric mindset across the organization, the tool must be relatively straight-forward to use and build upon. Also, it doesn’t really make sense to build out a proprietary visualization tool when time-series or bar charts powered by sql will suffice.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="database-role-topology">Database role topology<a class="hash-link" href="#database-role-topology" title="Direct link to heading">​</a></h3><p>When analysts and other dashboarding tools are allowed access to the same database as your various ETL tools, it is of utmost importance to grant them the minimum permissions necessary to perform the designated job function. In short, an analyst should never be able to inadvertently UPDATE event records coming from a production system, and a dashboarding tool should never be allowed the privileges necessary to issue a DROP TABLE statement. A flexible role topology allows the freedom for each of these groups to do what they need to do, but no more.</p><h1>System constraints</h1><p>There are many constraints to consider, and pipeline creation largely depends on data sources, deployment environment, associated networks, etc. If the source data/ production application is all on AWS, it doesn’t make sense to spin up a physical server in the office for the pipeline. AWS offers plenty of tools for moving data within the system itself (as well as the cost implications when keeping AWS-generated data inside AWS). Alternatively, if the application lives on a VM, hosted in the office, it makes sense to spin up another VM on the same subnet for pipelining. Since constraints are entirely dependent on the existing system in which to gather data from, it would simply be impossible to cover all them in a simple paragraph. That's where a little human creativity goes a long way.</p><h1>Conclusion</h1><p>In conclusion, there are a plethora of options to consider when building out a data pipelining system. There are many pros and cons to each alternative, and this document outlines some of the major factors involved. I hope it helps, and at the very least, provokes questions and thoughtful system design. If thought through from the start, many system inefficiencies can be avoided, and the power associated with efficient, reliable data collection can rapidly come to fruition.</p>]]></content>
        <author>
            <name>Jake</name>
        </author>
        <category label="data" term="data"/>
        <category label="pipeline" term="pipeline"/>
        <category label="design" term="design"/>
    </entry>
</feed>